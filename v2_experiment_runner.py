#!/usr/bin/env python3
"""
å¢å¼ºå‹HALT-CoTå®éªŒè¿è¡Œå™¨ - é‡æ„ç‰ˆ
èåˆLiu & Wang (2025)çš„ç­”æ¡ˆæ”¶æ•›æ£€æµ‹å’ŒLaaouachçš„ç†µåŸºæ—©åœæ–¹æ³•
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import time
import re
import numpy as np
import nltk
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple, NamedTuple
from collections import Counter
from dataclasses import dataclass

# ============================================================================
# é…ç½®å’Œè·¯å¾„å¸¸é‡
# ============================================================================
BASE_DIR = Path("/root/autodl-tmp")
CONFIG_DIR = BASE_DIR / "config"
DATA_DIR = BASE_DIR / "data"
RESULTS_DIR = BASE_DIR / "results"

RESULTS_DIR.mkdir(exist_ok=True)

# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================
@dataclass
class GenerationState:
    """ç”ŸæˆçŠ¶æ€ç®¡ç†"""
    tokens_used: int = 0
    full_response: str = ""
    full_sequence_ids: Optional[torch.Tensor] = None
    early_stopped: bool = False
    halt_reason: Optional[str] = None
    predicted_answer: Optional[str] = None
    
@dataclass 
class CheckpointResult:
    """æ£€æŸ¥ç‚¹ç»“æœ"""
    should_halt: bool = False
    halt_reason: Optional[str] = None
    answer: Optional[str] = None
    entropy: float = 0.0
    confidence: float = 0.0

# ============================================================================
# æ—©åœæ£€æµ‹å™¨
# ============================================================================
class AnswerConsistencyDetector:
    """ç­”æ¡ˆä¸€è‡´æ€§æ£€æµ‹å™¨ï¼ˆåŸºäºLiu & Wang 2025ï¼‰"""
    
    def __init__(self, k: int = 3):
        self.k = k
        self.answer_history = []
    
    def add_answer(self, answer: Optional[str]) -> bool:
        """æ·»åŠ ç­”æ¡ˆå¹¶æ£€æŸ¥æ”¶æ•›æ€§"""
        self.answer_history.append(answer)
        
        if len(self.answer_history) < self.k or answer is None:
            return False
        
        recent_answers = self.answer_history[-self.k:]
        return len(set(recent_answers)) == 1
    
    def reset(self):
        self.answer_history = []

class EntropyHaltDetector:
    """åŸºäºç†µçš„æ—©åœæ£€æµ‹å™¨ï¼ˆåŸºäºLaaouach HALT-CoTï¼‰"""
    
    def __init__(self, threshold: float = 0.6, consecutive_steps: int = 2):
        self.threshold = threshold
        self.consecutive_steps = consecutive_steps
        self.entropy_history = []
        self.low_entropy_count = 0
    
    def should_halt(self, entropy: float) -> Tuple[bool, float]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢"""
        self.entropy_history.append(entropy)
        
        if entropy < self.threshold:
            self.low_entropy_count += 1
        else:
            self.low_entropy_count = 0
        
        should_stop = self.low_entropy_count >= self.consecutive_steps
        return should_stop, entropy
    
    def reset(self):
        self.entropy_history = []
        self.low_entropy_count = 0

# ============================================================================
# ç­”æ¡ˆæå–å™¨
# ============================================================================
class AnswerExtractor:
    """æ”¹è¿›çš„ç­”æ¡ˆæå–å™¨"""
    
    @staticmethod
    def extract_answer(text: str, strict: bool = False) -> Optional[str]:
        """æ”¹è¿›çš„ç­”æ¡ˆæå– - ä½¿ç”¨æ›´é²æ£’çš„ç­–ç•¥"""
        
        # ç¬¬ä¸€å±‚ï¼šæœ€é«˜ä¼˜å…ˆçº§æ ¼å¼
        high_confidence_patterns = [
            r'####\s*(-?\d+(?:,\d+)*(?:\.\d+)?)',  # GSM8Kæ ‡å‡†
            r'\\boxed\{([^}]+)\}',  # å®Œæ•´çš„boxedï¼ˆæ³¨æ„è¿™é‡Œæ”¹ä¸ºéè´ªå©ªåŒ¹é…å•å±‚å¤§æ‹¬å·å†…å®¹ï¼‰
        ]
        
        for pattern in high_confidence_patterns:
            matches = list(re.finditer(pattern, text, re.I))
            if matches:
                answer = matches[-1].group(1).replace(',', '').strip()
                if answer:
                    # å¤„ç† LaTeX åˆ†æ•°
                    frac_match = re.match(r'\\frac\{(\d+)\}\{(\d+)\}', answer)
                    if frac_match:
                        return frac_match.group(1)  # åªè¿”å›åˆ†å­ï¼Œæˆ–è€…æ”¹ä¸º f"{åˆ†å­}/{åˆ†æ¯}"
                    return answer
        
        # ===== æ”¹è¿›ï¼šå¤„ç†ä¸å®Œæ•´çš„ boxed =====
        if '\\boxed{' in text and not strict:
            last_boxed_pos = text.rfind('\\boxed{')
            content_after = text[last_boxed_pos + 7:]
            
            # å°è¯•æå–åˆ°é—­æ‹¬å·
            brace_count = 1
            answer_content = ""
            
            for i, char in enumerate(content_after):
                if char == '{':
                    brace_count += 1
                    answer_content += char
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        if answer_content.strip():
                            # å¤„ç† LaTeX åˆ†æ•° \frac{a}{b}
                            frac_match = re.match(r'\\frac\{(\d+)\}\{(\d+)\}', answer_content)
                            if frac_match:
                                # æ ¹æ®éœ€æ±‚è¿”å›åˆ†å­æˆ–åˆ†æ•°
                                return frac_match.group(1)  # åªè¿”å›åˆ†å­
                            
                            # æå–çº¯æ•°å­—
                            num_match = re.search(r'-?\d+(?:\.\d+)?', answer_content)
                            if num_match:
                                return num_match.group(0)
                        break
                    answer_content += char
                else:
                    answer_content += char
            
            # å¦‚æœæ²¡æ‰¾åˆ°é—­æ‹¬å·ï¼Œæ™ºèƒ½æå–
            if brace_count > 0 and answer_content:
                for end_marker in ['\n', '\\text', 'Therefore', 'Thus']:
                    if end_marker in answer_content:
                        answer_content = answer_content[:answer_content.index(end_marker)]
                        break
                
                answer_content = answer_content.strip()
                
                # å¤„ç† LaTeX åˆ†æ•°
                frac_match = re.match(r'\\frac\{(\d+)\}\{(\d+)\}', answer_content)
                if frac_match:
                    return frac_match.group(1)
                
                # æå–çº¯æ•°å­—
                if len(answer_content) < 50:
                    num_match = re.search(r'-?\d+(?:\.\d+)?', answer_content)
                    if num_match:
                        return num_match.group(0)
        
        # ä¸¥æ ¼æ¨¡å¼ä¸‹åªä¿¡ä»»é«˜ä¼˜å…ˆçº§æ ¼å¼
        if strict:
            return None
        
        # ç¬¬äºŒå±‚ï¼šå¸¦æœ‰"final answer"çš„æ˜ç¡®å£°æ˜
        final_answer_patterns = [
            r'final answer is[:\s]+\$?(-?\d+(?:,\d+)*(?:\.\d+)?)',
            r'answer is[:\s]+\$?(-?\d+(?:,\d+)*(?:\.\d+)?)',
            r'Therefore,?\s+the answer is[:\s]+\$?(-?\d+(?:,\d+)*(?:\.\d+)?)',
        ]
        
        for pattern in final_answer_patterns:
            matches = list(re.finditer(pattern, text, re.I))
            if matches:
                answer = matches[-1].group(1).replace(',', '').strip()
                if answer:
                    return answer
        
        # ç¬¬ä¸‰å±‚ï¼šä»ç­‰å¼ä¸­æå–ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
        return _extract_from_equations(text)


def _extract_from_equations(text: str) -> Optional[str]:
    """ä»ç­‰å¼ä¸­æ™ºèƒ½æå–ç­”æ¡ˆï¼ˆåŸé€»è¾‘ï¼‰"""
    lines = text.split('\n')
    keywords = ['total', 'answer', 'result', 'value', 'earnings', 'profit', 'money']
    
    # å€’åºéå†ï¼Œä¼˜å…ˆæ‰¾æœ€åçš„ç­‰å¼
    for line in reversed(lines):
        line_lower = line.lower()
        if any(k in line_lower for k in keywords) and '=' in line:
            rhs = line.split('=')[-1].strip()
            # ç¡®ä¿ä¸åŒ…å«è¿ç®—ç¬¦ï¼ˆä¸æ˜¯ä¸­é—´è®¡ç®—ï¼‰
            if not (re.search(r'[+*/]', rhs) or re.search(r'\s-\s', rhs)):
                num_match = re.search(r'^\$?(-?\d+(?:,\d+)*(?:\.\d+)?)', rhs)
                if num_match:
                    return num_match.group(1).replace(',', '')
    
    return None
    
    @staticmethod
    def _extract_incomplete_boxed(text: str) -> Optional[str]:
        """æå–ä¸å®Œæ•´çš„boxedç­”æ¡ˆ - æ”¹è¿›ç‰ˆ"""
        last_boxed_pos = text.rfind('\\boxed{')
        content_after = text[last_boxed_pos + 7:]
        
        # å°è¯•æå–åˆ°é—­æ‹¬å·
        brace_count = 1
        answer_content = ""
        
        for i, char in enumerate(content_after):
            if char == '{':
                brace_count += 1
                answer_content += char
            elif char == '}':
                brace_count -= 1
                if brace_count == 0:
                    if answer_content.strip():
                        # å¤„ç† LaTeX åˆ†æ•° \frac{a}{b}
                        frac_match = re.match(r'\\frac\{(\d+)\}\{(\d+)\}', answer_content)
                        if frac_match:
                            # è¿”å›åˆ†æ•°å½¢å¼æˆ–å°æ•°
                            numerator = int(frac_match.group(1))
                            denominator = int(frac_match.group(2))
                            return f"{numerator}/{denominator}"
                        
                        # æå–çº¯æ•°å­—
                        num_match = re.search(r'-?\d+(?:\.\d+)?', answer_content)
                        if num_match:
                            return num_match.group(0)
                    break
                answer_content += char
            else:
                answer_content += char
        
        # å¦‚æœæ²¡æ‰¾åˆ°é—­æ‹¬å·ï¼Œæ™ºèƒ½æå–
        if brace_count > 0 and answer_content:
            # æå–åˆ°ç¬¬ä¸€ä¸ªä¸åˆç†çš„ä½ç½®
            for end_marker in ['\n', '\\text', 'Therefore', 'Thus']:
                if end_marker in answer_content:
                    answer_content = answer_content[:answer_content.index(end_marker)]
                    break
            
            answer_content = answer_content.strip()
            
            # å¤„ç† LaTeX åˆ†æ•°
            frac_match = re.match(r'\\frac\{(\d+)\}\{(\d+)\}', answer_content)
            if frac_match:
                numerator = int(frac_match.group(1))
                denominator = int(frac_match.group(2))
                return f"{numerator}/{denominator}"
            
            # æå–çº¯æ•°å­—
            if len(answer_content) < 50:
                num_match = re.search(r'-?\d+(?:\.\d+)?', answer_content)
                if num_match:
                    return num_match.group(0)
        
        return None

# ============================================================================
# æ¢é’ˆç³»ç»Ÿ
# ============================================================================
class SmartProbeSystem:
    """æ™ºèƒ½æ¢é’ˆç³»ç»Ÿ - è¯†åˆ«æ¨ç†é˜¶æ®µå¹¶é€‰æ‹©æ€§æ¢æµ‹"""
    
    def __init__(self, model, tokenizer, debug: bool = False):
        self.model = model
        self.tokenizer = tokenizer
        self.debug = debug
        
        # æ¨ç†é˜¶æ®µæ ‡è®°è¯
        self.reasoning_markers = {
            'calculation': ['=', 'equals', 'total', 'sum', 'result'],
            'conclusion': ['therefore', 'thus', 'so', 'hence', 'finally'],
            'intermediate': ['step', 'first', 'next', 'then', 'now'],
            'answer_signal': ['answer is', 'answer:', '####', '\\boxed', 'final answer']
        }
    
    def identify_reasoning_stage(self, text: str) -> str:
        """è¯†åˆ«å½“å‰æ¨ç†é˜¶æ®µ"""
        text_lower = text.lower()
        last_200_chars = text_lower[-200:]  # åªçœ‹æœ€è¿‘çš„æ–‡æœ¬
        
        # ä¼˜å…ˆçº§ï¼šç­”æ¡ˆä¿¡å· > ç»“è®º > è®¡ç®— > ä¸­é—´æ­¥éª¤
        for marker in self.reasoning_markers['answer_signal']:
            if marker in last_200_chars:
                return 'answer_signal'
        
        for marker in self.reasoning_markers['conclusion']:
            if marker in last_200_chars:
                return 'conclusion'
        
        for marker in self.reasoning_markers['calculation']:
            if marker in last_200_chars:
                return 'calculation'
        
        for marker in self.reasoning_markers['intermediate']:
            if marker in last_200_chars:
                return 'intermediate'
        
        return 'unknown'
    
    def should_probe_at_stage(self, stage: str) -> bool:
        """åˆ¤æ–­è¯¥é˜¶æ®µæ˜¯å¦åº”è¯¥æ¢æµ‹"""
        # ç­”æ¡ˆä¿¡å·ï¼šå¿…é¡»æ¢æµ‹
        if stage == 'answer_signal':
            return True
        
        # ç»“è®ºé˜¶æ®µï¼šé«˜ä¼˜å…ˆçº§æ¢æµ‹
        if stage == 'conclusion':
            return True
        
        # è®¡ç®—é˜¶æ®µï¼šä¸­ç­‰ä¼˜å…ˆçº§ï¼ˆå¯èƒ½æ˜¯ä¸­é—´ç»“æœï¼‰
        if stage == 'calculation':
            return True
        
        # ä¸­é—´æ­¥éª¤ï¼šä½ä¼˜å…ˆçº§ï¼ˆé€šå¸¸ä¸æ¢æµ‹ï¼‰
        if stage == 'intermediate':
            return False
        
        return False
    
    def detect_answer_in_context(self, text: str) -> Optional[str]:
        """ç›´æ¥ä»ä¸Šä¸‹æ–‡æ£€æµ‹ç­”æ¡ˆï¼ˆæ— éœ€æ¢é’ˆï¼‰"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ˜ç¡®ç­”æ¡ˆæ ¼å¼
        if '####' in text:
            match = re.search(r'####\s*(-?\d+(?:,\d+)*(?:\.\d+)?)', text)
            if match:
                return match.group(1).replace(',', '')
        
        if '\\boxed{' in text:
            match = re.search(r'\\boxed\{(-?\d+(?:,\d+)*(?:\.\d+)?)\}', text)
            if match:
                return match.group(1).replace(',', '')
        
        return None
    
    def create_probe_prompt(self, stage: str) -> str:
        """æ ¹æ®æ¨ç†é˜¶æ®µåˆ›å»ºåˆé€‚çš„æ¢é’ˆæç¤º"""
        prompts = {
            'answer_signal': "\n#### ",  # GSM8Kæ ‡å‡†æ ¼å¼
            'conclusion': "\n\nTherefore, the final answer is: ",
            'calculation': "\n\nThe result of this calculation is: ",
            'intermediate': "\n\nThe current value is: "
        }
        return prompts.get(stage, "\n#### ")
    
    def probe_answer(
        self, 
        full_sequence_ids: torch.Tensor, 
        current_text: str,
        stage: Optional[str] = None
    ) -> CheckpointResult:
        """æ™ºèƒ½æ¢é’ˆ - æ ¹æ®æ¨ç†é˜¶æ®µè°ƒæ•´ç­–ç•¥"""
        
        # 1. å…ˆå°è¯•ç›´æ¥ä»ä¸Šä¸‹æ–‡æå–ï¼ˆæœ€å¿«ï¼‰
        context_answer = self.detect_answer_in_context(current_text)
        if context_answer:
            if self.debug:
                print(f"      âœ“ ç›´æ¥æå–: {context_answer}")
            return CheckpointResult(
                answer=context_answer,
                entropy=0.1,
                confidence=0.95
            )
        
        # 2. è¯†åˆ«æ¨ç†é˜¶æ®µ
        if stage is None:
            stage = self.identify_reasoning_stage(current_text)
        
        if self.debug:
            print(f"      ğŸ¯ æ¨ç†é˜¶æ®µ: {stage}")
        
        # 3. æ ¹æ®é˜¶æ®µå†³å®šæ˜¯å¦æ¢æµ‹
        if not self.should_probe_at_stage(stage):
            return CheckpointResult(
                should_halt=False,
                answer=None,
                entropy=100.0
            )
        
        # 4. æ‰§è¡Œæ¢é’ˆ
        try:
            probe_text = self.create_probe_prompt(stage)
            probe_tokens = self.tokenizer.encode(
                probe_text, 
                return_tensors='pt', 
                add_special_tokens=False
            ).to(self.model.device)
            
            probe_input_ids = torch.cat([full_sequence_ids, probe_tokens], dim=-1)
            
            # æ ¹æ®é˜¶æ®µè°ƒæ•´ç”Ÿæˆå‚æ•°
            max_new_tokens = 30 if stage == 'answer_signal' else 20
            
            with torch.no_grad():
                gen_output = self.model.generate(
                    probe_input_ids,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    output_scores=True,
                    return_dict_in_generate=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # 5. è§£æç»“æœ
            if gen_output.sequences.shape[1] > probe_input_ids.shape[1]:
                answer_tokens = gen_output.sequences[0][probe_input_ids.shape[1]:]
                probe_response = self.tokenizer.decode(
                    answer_tokens, 
                    skip_special_tokens=True
                ).strip()
                
                if self.debug:
                    print(f"      ğŸ“ æ¢é’ˆå“åº”: '{probe_response.replace('\n', '  ')}'")
                
                # ä½¿ç”¨å®Œæ•´çš„ç­”æ¡ˆæå–å™¨
                answer = AnswerExtractor.extract_answer(probe_response)
                
                # å¦‚æœæå–å¤±è´¥ï¼Œå°è¯•ç®€å•æ•°å­—æå–
                if not answer:
                    answer = self._extract_number_fallback(probe_response)
                
                if self.debug:
                    print(f"      ğŸ”¢ æå–ç­”æ¡ˆ: '{answer}'")
                
                # è®¡ç®—ç†µï¼ˆä»…ç”¨ç¬¬ä¸€ä¸ªtokençš„ç†µï¼‰
                entropy = 100.0
                confidence = 0.0
                
                if gen_output.scores and len(gen_output.scores) > 0:
                    first_token_logits = gen_output.scores[0][0]
                    probs = torch.softmax(first_token_logits, dim=-1)
                    log_probs = torch.log_softmax(first_token_logits, dim=-1)
                    entropy = -torch.sum(probs * log_probs).item()
                    confidence = 1.0 / (1.0 + entropy)
                
                return CheckpointResult(
                    should_halt=False,
                    answer=answer,
                    entropy=entropy,
                    confidence=confidence
                )
        
        except Exception as e:
            if self.debug:
                print(f"      âŒ æ¢é’ˆé”™è¯¯: {e}")
        
        return CheckpointResult(entropy=100.0)
    
    def _extract_number_fallback(self, text: str) -> Optional[str]:
        """åå¤‡æ•°å­—æå–æ–¹æ¡ˆ"""
        # å°è¯•æå–å®Œæ•´æ•°å­—ï¼ˆæ”¯æŒé€—å·åˆ†éš”ï¼‰
        match = re.search(r'-?\d+(?:,\d{3})*(?:\.\d+)?', text)
        if match:
            return match.group(0).replace(',', '')
        
        # å°è¯•æå–ä»»ä½•æ•°å­—åºåˆ—
        match = re.search(r'-?\d+\.?\d*', text)
        if match:
            return match.group(0)
        
        return None

# ============================================================================
# ç”Ÿæˆç®¡ç†å™¨
# ============================================================================
class GenerationManager:
    """æ”¹è¿›çš„ç”Ÿæˆç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.stop_words = ["Human:", "User:", "\n\nHuman", "\n\nUser", "Observation:", "Question:"]
        
    def should_stop_naturally(self, text: str, new_token_id: int, tokenizer) -> Tuple[bool, str]:
        """æ”¹è¿›çš„è‡ªç„¶åœæ­¢æ£€æµ‹ - æ›´ä¿å®ˆã€æ›´å‡†ç¡®"""
        
        # 1. EOS token
        if new_token_id == tokenizer.eos_token_id:
            return True, "eos_token"

        # 2. åœæ­¢è¯æ£€æŸ¥
        for stop_word in self.stop_words:
            if stop_word in text:
                return True, f"stop_word_{stop_word}"

        # 3. ===== æ”¹è¿›çš„ Boxed ç­”æ¡ˆæ£€æµ‹ =====
        if "\\boxed{" in text:
            last_boxed_pos = text.rfind("\\boxed{")
            content_after_boxed = text[last_boxed_pos + 7:]
            
            # è®¡ç®—å¤§æ‹¬å·åµŒå¥—
            brace_count = 1
            closed_pos = -1
            
            for i, char in enumerate(content_after_boxed):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        closed_pos = i
                        break
            
            # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„é—­æ‹¬å·
            if closed_pos != -1:
                remaining = content_after_boxed[closed_pos + 1:].strip()
                remaining = remaining.lstrip('.,;:!?\n\r\t ')
                
                # åé¢æ²¡æœ‰å®è´¨å†…å®¹æ‰åœæ­¢
                if len(remaining) == 0:
                    return True, "boxed_answer_complete"
                
                if len(remaining) < 30:
                    alnum_count = sum(1 for c in remaining if c.isalnum())
                    if alnum_count < 5:
                        return True, "boxed_answer_complete"

        # 4. ===== æ”¹è¿›ï¼šåªæ£€æµ‹æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆæ ‡è®° =====
        final_answer_markers = [
            ("#### ", 20),  # GSM8Kæ ‡å‡†ç­”æ¡ˆæ ¼å¼
            ("The final answer is", 30),
            ("Therefore, the final answer is", 30),
            ("Thus, the final answer is", 30),
        ]
        
        for marker, min_length_after in final_answer_markers:
            if marker in text:
                marker_pos = text.rfind(marker)
                text_after_marker = text[marker_pos:]
                
                # å¿…é¡»æœ‰è¶³å¤Ÿçš„å†…å®¹
                if len(text_after_marker) > len(marker) + min_length_after:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ boxed ç­”æ¡ˆ
                    if '\\boxed{' in text_after_marker:
                        boxed_content = text_after_marker[text_after_marker.rfind('\\boxed{') + 7:]
                        if '}' in boxed_content:
                            return True, f"final_marker_with_boxed"
                    
                    # æˆ–è€…æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„æ•°å­—ç­”æ¡ˆ
                    elif re.search(r'\d+', text_after_marker):
                        # ç¡®ä¿ç­”æ¡ˆåæœ‰å¥å·æˆ–åŒæ¢è¡Œ
                        if ('.' in text_after_marker and 
                            not any(word in text_after_marker.lower() for word in ['step', 'then', 'next'])):
                            return True, f"final_marker_with_number"

        # 5. æ£€æµ‹å¼‚å¸¸æ¨¡å¼
        abnormal_patterns = [
            "Human:", "Assistant:", "You are an AI", "I am Claude",
        ]
        text_lower = text.lower()
        for pattern in abnormal_patterns:
            if pattern.lower() in text_lower:
                pattern_pos = text_lower.rfind(pattern.lower())
                if pattern_pos > len(text) * 0.5:
                    return True, f"abnormal_pattern"

        # 6. æ£€æµ‹é‡å¤å†…å®¹
        if len(text) > 300:
            last_200 = text[-200:]
            prev_200 = text[-400:-200] if len(text) > 400 else ""
            if prev_200 and last_200 == prev_200:
                return True, "exact_repetition"

        # 7. æ£€æµ‹è¿‡é•¿ç”Ÿæˆï¼ˆå®‰å…¨æªæ–½ï¼‰
        if len(text) > 2500:
            return True, "max_length_safety"

        return False, ""
    def _is_likely_final_answer(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯æœ€ç»ˆç­”æ¡ˆ - æ›´ä¸¥æ ¼çš„æ ‡å‡†"""
        
        # è·å–æœ€å300ä¸ªå­—ç¬¦
        tail = text[-300:] if len(text) > 300 else text
        
        # å¿…é¡»åŒæ—¶æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
        # 1. åŒ…å« "answer" æˆ– "####" æˆ– "\boxed"
        has_answer_signal = any(marker in tail.lower() for marker in 
                                ['#### ', 'final answer', 'the answer is', '\\boxed{'])
        
        if not has_answer_signal:
            return False
        
        # 2. ç­”æ¡ˆä¿¡å·åé¢æœ‰æ•°å­—
        lines = tail.split('\n')
        for i, line in enumerate(lines):
            line_lower = line.lower()
            if any(marker in line_lower for marker in ['answer', '####', '\\boxed']):
                # æ£€æŸ¥è¿™ä¸€è¡Œæˆ–åç»­å‡ è¡Œæ˜¯å¦æœ‰æ•°å­—
                remaining_lines = '\n'.join(lines[i:i+3])
                if re.search(r'\d+', remaining_lines):
                    # 3. ç¡®ä¿ä¸æ˜¯åœ¨è®¡ç®—è¿‡ç¨‹ä¸­
                    # å¦‚æœåé¢è¿˜æœ‰ "Step", "Next", "Then" ç­‰è¯ï¼Œè¯´æ˜è¿˜åœ¨æ¨ç†
                    if not any(word in remaining_lines.lower() for word in 
                              ['step', 'next', 'then', 'now let', 'we need to']):
                        return True
        
        return False
# ============================================================================
# æ—©åœå†³ç­–å™¨
# ============================================================================
class SmartHaltDecisionMaker:
    """æ™ºèƒ½æ—©åœå†³ç­–å™¨ - ç»“åˆæ¨ç†é˜¶æ®µ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.use_consistency = config.get('early_stopping', {}).get('use_answer_consistency', True)
        self.use_entropy = config.get('early_stopping', {}).get('use_entropy_halt', True)
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.consistency_detector = AnswerConsistencyDetector(
            k=config.get('early_stopping', {}).get('consistency_k', 3)
        )
        self.entropy_detector = EntropyHaltDetector(
            threshold=config.get('early_stopping', {}).get('entropy_threshold', 0.6),
            consecutive_steps=config.get('early_stopping', {}).get('entropy_consecutive_steps', 2)
        )
        
        self.last_check_token_count = 0
        self.last_stage = None
        self.stage_check_counts = Counter()
    
    def should_check_now(
        self, 
        full_text: str, 
        tokens_used: int, 
        stage: str,
        cooldown: int = 40
    ) -> bool:
        """æ™ºèƒ½æ£€æŸ¥åˆ¤æ–­ - è€ƒè™‘æ¨ç†é˜¶æ®µ"""
        
        # å¦‚æœæ—©åœåŠŸèƒ½å®Œå…¨ç¦ç”¨ï¼Œä¸éœ€è¦æ£€æŸ¥
        if not self.use_consistency and not self.use_entropy:
            return False
        
        # å¦‚æœåœ¨ç­”æ¡ˆä¿¡å·é˜¶æ®µ,ç«‹å³æ£€æŸ¥
        if stage == 'answer_signal':
            return True
        
        # å¦‚æœåœ¨ç»“è®ºé˜¶æ®µ,ä¸”è·ç¦»ä¸Šæ¬¡æ£€æŸ¥è¶…è¿‡20ä¸ªtoken
        if stage == 'conclusion' and (tokens_used - self.last_check_token_count) >= 20:
            return True
        
        # å¦‚æœåœ¨è®¡ç®—é˜¶æ®µ,ä½¿ç”¨æ­£å¸¸å†·å´
        if stage == 'calculation':
            if (tokens_used - self.last_check_token_count) >= cooldown:
                # é™åˆ¶åŒä¸€é˜¶æ®µçš„æ£€æŸ¥æ¬¡æ•°
                if self.stage_check_counts.get(stage, 0) < 3:
                    return True
        
        # ä¸­é—´æ­¥éª¤é˜¶æ®µ,æ›´é•¿çš„å†·å´æ—¶é—´
        if stage == 'intermediate':
            return (tokens_used - self.last_check_token_count) >= cooldown * 2
        
        return False
    
    def update_check_state(self, tokens_used: int, stage: str):
        """æ›´æ–°æ£€æŸ¥çŠ¶æ€"""
        self.last_check_token_count = tokens_used
        if stage != self.last_stage:
            self.stage_check_counts[stage] = 0
            self.last_stage = stage
        self.stage_check_counts[stage] += 1
    
    def make_decision(
        self, 
        probe_result: CheckpointResult, 
        stage: str
    ) -> CheckpointResult:
        """æ™ºèƒ½å†³ç­– - è€ƒè™‘æ¨ç†é˜¶æ®µå’Œé…ç½®"""
        
        # å¦‚æœæ—©åœåŠŸèƒ½å®Œå…¨ç¦ç”¨,ç›´æ¥è¿”å›ä¸åœæ­¢
        if not self.use_consistency and not self.use_entropy:
            return CheckpointResult(
                should_halt=False,
                halt_reason=None,
                answer=probe_result.answer,
                entropy=probe_result.entropy,
                confidence=probe_result.confidence
            )
        
        if not probe_result.answer:
            return probe_result
        
        # ç­”æ¡ˆä¿¡å·é˜¶æ®µçš„å†³ç­–æ›´æ¿€è¿›
        if stage == 'answer_signal':
            # åªæœ‰åœ¨å¯ç”¨ç†µæ£€æµ‹æ—¶æ‰ä½¿ç”¨ç†µåˆ¤æ–­
            if self.use_entropy and probe_result.entropy < 0.5:
                return CheckpointResult(
                    should_halt=True,
                    halt_reason=f"answer_signal_high_confidence",
                    answer=probe_result.answer,
                    entropy=probe_result.entropy,
                    confidence=probe_result.confidence
                )
        
        # ç»“è®ºé˜¶æ®µ - ä¸­ç­‰æ¿€è¿›
        if stage == 'conclusion':
            # ä¸€è‡´æ€§æ£€æµ‹
            if self.use_consistency:
                is_consistent = self.consistency_detector.add_answer(probe_result.answer)
                if is_consistent and (not self.use_entropy or probe_result.entropy < 0.8):
                    return CheckpointResult(
                        should_halt=True,
                        halt_reason=f"conclusion_consistency",
                        answer=probe_result.answer,
                        entropy=probe_result.entropy,
                        confidence=probe_result.confidence
                    )
            
            # æä½ç†µä¹Ÿå¯ä»¥åœ¨ç»“è®ºé˜¶æ®µåœæ­¢(ä»…å½“å¯ç”¨ç†µæ£€æµ‹)
            if self.use_entropy and probe_result.entropy < 0.3:
                return CheckpointResult(
                    should_halt=True,
                    halt_reason=f"conclusion_high_confidence",
                    answer=probe_result.answer,
                    entropy=probe_result.entropy,
                    confidence=probe_result.confidence
                )
        
        # è®¡ç®—é˜¶æ®µ - ä¿å®ˆç­–ç•¥
        if stage == 'calculation':
            # éœ€è¦åŒæ—¶æ»¡è¶³ç†µæ£€æµ‹å’Œä¸€è‡´æ€§æ£€æµ‹(å¦‚æœéƒ½å¯ç”¨)
            if self.use_entropy and probe_result.entropy < 0.15:
                # å¦‚æœå¯ç”¨äº†ä¸€è‡´æ€§æ£€æµ‹,éœ€è¦åŒæ—¶æ»¡è¶³
                if self.use_consistency:
                    is_consistent = self.consistency_detector.add_answer(probe_result.answer)
                    if is_consistent:
                        return CheckpointResult(
                            should_halt=True,
                            halt_reason=f"calculation_high_confidence_consistent",
                            answer=probe_result.answer,
                            entropy=probe_result.entropy,
                            confidence=probe_result.confidence
                        )
                else:
                    # å¦‚æœåªå¯ç”¨ç†µæ£€æµ‹,ç›´æ¥åœæ­¢
                    return CheckpointResult(
                        should_halt=True,
                        halt_reason=f"calculation_high_confidence",
                        answer=probe_result.answer,
                        entropy=probe_result.entropy,
                        confidence=probe_result.confidence
                    )
        
        # è®°å½•ç­”æ¡ˆä½†ä¸åœæ­¢(ä»…å½“å¯ç”¨ä¸€è‡´æ€§æ£€æµ‹)
        if self.use_consistency:
            self.consistency_detector.add_answer(probe_result.answer)
        
        return CheckpointResult(
            should_halt=False,
            halt_reason=None,
            answer=probe_result.answer,
            entropy=probe_result.entropy,
            confidence=probe_result.confidence
        )
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self.consistency_detector.reset()
        self.entropy_detector.reset()
        self.last_check_token_count = 0
        self.last_stage = None
        self.stage_check_counts.clear()


# ============================================================================
# ä¸»å®éªŒè¿è¡Œå™¨
# ============================================================================
class ExperimentRunner:
    """ä½¿ç”¨æ™ºèƒ½æ¢é’ˆçš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self):
        self.config = self.load_config()
        self.generation_manager = GenerationManager(self.config)
        self.halt_decision_maker = SmartHaltDecisionMaker(self.config)
        
        # æ·»åŠ è°ƒè¯•æ¨¡å¼é…ç½®
        self.debug_mode = self.config.get('experiment', {}).get('debug_probe', False)
        
        print(f"ğŸ”§ æ™ºèƒ½æ¢é’ˆé…ç½®: ç­”æ¡ˆä¸€è‡´æ€§={self.halt_decision_maker.use_consistency}, "
              f"ç†µæ£€æµ‹={self.halt_decision_maker.use_entropy}, "
              f"è°ƒè¯•æ¨¡å¼={self.debug_mode}")
    
    
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = CONFIG_DIR / "config.json"
        
        if not config_file.exists():
            default_config = {
                "active_model": "qwen",
                "model_configs": {
                    "qwen": {"name": "Qwen/Qwen2.5-7B-Instruct"}
                },
                "paths": {
                    "test_data": str(DATA_DIR / "gsm8k_test.json")
                },
                "experiment": {
                    "sample_size": 10,
                    "max_new_tokens": 512,
                    "do_sample": False,
                    "temperature": 0.7,
                    "save_results": True,
                    "verbose": False
                },
                "early_stopping": {
                    "use_answer_consistency": True,
                    "use_entropy_halt": True,
                    "consistency_k": 3,
                    "entropy_threshold": 0.6,
                    "entropy_consecutive_steps": 2,
                    "min_tokens_before_check": 100,
                    "cooldown_tokens": 40
                }
            }
            config_file.parent.mkdir(exist_ok=True)
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
            return default_config
        
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def load_test_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        test_file = Path(self.config['paths']['test_data'])
        if not test_file.exists():
            test_file = DATA_DIR / "gsm8k_test.json"
            if not test_file.exists():
                raise FileNotFoundError("æµ‹è¯•æ•°æ®ä¸å­˜åœ¨")
        
        with open(test_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        sample_size = self.config['experiment']['sample_size']
        if len(data) > sample_size:
            data = data[:sample_size]
        
        print(f"âœ… å·²åŠ è½½ {len(data)} æ¡æµ‹è¯•æ•°æ®")
        return data
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        model_key = self.config['active_model']
        model_name = self.config['model_configs'][model_key]['name']
        
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")
        return tokenizer, model
    
    def get_ground_truth(self, item: Dict[str, Any]) -> Optional[str]:
        """è·å–æ ‡å‡†ç­”æ¡ˆ"""
        if 'numerical_answer' in item and item['numerical_answer']:
            return str(item['numerical_answer'])
        
        if 'answer' in item:
            return AnswerExtractor.extract_answer(item['answer'])
        
        return None

    def run_single_experiment(
        self, 
        tokenizer, 
        model, 
        question: str, 
        ground_truth: str, 
        sample_id: int
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå®éªŒ - ä½¿ç”¨æ™ºèƒ½æ¢é’ˆ"""
        
        prompt = f"""Question: {question}

Please solve this step by step and provide your final answer.

Answer:"""
        
        print(f"\nğŸ“ æ ·æœ¬ {sample_id + 1}: {question[:80]}...")
        start_time = time.time()
        
        # åˆå§‹åŒ–çŠ¶æ€å’Œç³»ç»Ÿ
        state = GenerationState()
        probe_system = SmartProbeSystem(model, tokenizer, debug=self.debug_mode)  # ä½¿ç”¨æ™ºèƒ½æ¢é’ˆ
        self.halt_decision_maker.reset()
        
        # å‡†å¤‡è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        input_ids = inputs['input_ids'].to(model.device)
        attention_mask = inputs['attention_mask'].to(model.device)
        
        state.full_sequence_ids = input_ids.clone()
        
        # ç”Ÿæˆå‚æ•°
        exp_config = self.config.get('experiment', {})
        max_new_tokens = exp_config.get('max_new_tokens', 512)
        temperature = exp_config.get('temperature', 0.7)
        do_sample = exp_config.get('do_sample', False)
        
        gen_kwargs = {
            "do_sample": do_sample,
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
        }
        if do_sample:
            gen_kwargs["temperature"] = temperature
        
        try:
            # ä¸»ç”Ÿæˆå¾ªç¯
            past_key_values = None
            current_input_ids = input_ids
            entropy_values = []
            stage_history = []
            
            while state.tokens_used < max_new_tokens and not state.early_stopped:
                # æ¨¡å‹å‰å‘ä¼ æ’­
                with torch.no_grad():
                    outputs = model(
                        input_ids=current_input_ids,
                        past_key_values=past_key_values,
                        use_cache=True,
                        attention_mask=attention_mask if past_key_values is None else None
                    )
                
                # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
                next_token_logits = outputs.logits[:, -1, :]
                past_key_values = outputs.past_key_values
                
                if do_sample:
                    probs = torch.softmax(next_token_logits / temperature, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)
                
                # æ›´æ–°çŠ¶æ€
                state.full_sequence_ids = torch.cat([state.full_sequence_ids, next_token], dim=-1)
                state.tokens_used += 1
                current_input_ids = next_token
                
                # è§£ç æ–°token
                new_token_id = next_token.item()
                new_text = tokenizer.decode([new_token_id], skip_special_tokens=True)
                state.full_response += new_text
                
                # æ£€æŸ¥è‡ªç„¶åœæ­¢æ¡ä»¶
                should_stop, stop_reason = self.generation_manager.should_stop_naturally(
                    state.full_response, new_token_id, tokenizer
                )
                if should_stop:
                    print(f"   ğŸ›‘ è‡ªç„¶åœæ­¢: {stop_reason}")
                    break
                
                # æ™ºèƒ½æ£€æŸ¥ç‚¹åˆ¤æ–­
                min_tokens = self.config.get('early_stopping', {}).get('min_tokens_before_check', 100)
                
                if state.tokens_used >= min_tokens:
                    # è¯†åˆ«å½“å‰æ¨ç†é˜¶æ®µ
                    current_stage = probe_system.identify_reasoning_stage(state.full_response)
                    stage_history.append(current_stage)
                    
                    # åˆ¤æ–­æ˜¯å¦åº”è¯¥æ£€æŸ¥
                    cooldown = self.config.get('early_stopping', {}).get('cooldown_tokens', 40)
                    
                    if self.halt_decision_maker.should_check_now(
                        state.full_response, 
                        state.tokens_used, 
                        current_stage,
                        cooldown
                    ):
                        # æ‰§è¡Œæ™ºèƒ½æ¢é’ˆ
                        probe_result = probe_system.probe_answer(
                            state.full_sequence_ids,
                            state.full_response,
                            current_stage
                        )
                        
                        # æ›´æ–°æ£€æŸ¥çŠ¶æ€
                        self.halt_decision_maker.update_check_state(state.tokens_used, current_stage)
                        
                        if probe_result.answer:
                            clean_context = state.full_response[-100:].replace('\n', 'â')
                            print(f"   ğŸ” [æ£€æŸ¥ç‚¹@{current_stage}] Tokens: {state.tokens_used}")
                            print(f"      ğŸ“„ ä¸Šä¸‹æ–‡: ...{clean_context}")
                            print(f"      ğŸ§ª æ¢é’ˆ: '{probe_result.answer}' | ç†µ: {probe_result.entropy:.4f}")
                            
                            entropy_values.append(probe_result.entropy)
                        
                        # æ™ºèƒ½å†³ç­–
                        decision = self.halt_decision_maker.make_decision(probe_result, current_stage)
                        
                        if decision.should_halt:
                            state.early_stopped = True
                            state.halt_reason = decision.halt_reason
                            state.predicted_answer = decision.answer
                            print(f"   ğŸ›‘ [æ—©åœ] {decision.halt_reason} | ç­”æ¡ˆ: {decision.answer}")
                            break
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            clean_response = state.full_response
            for stop_word in self.generation_manager.stop_words:
                if stop_word in clean_response:
                    clean_response = clean_response.split(stop_word)[0].strip()
            
            # æå–æœ€ç»ˆç­”æ¡ˆ
            if not state.predicted_answer:
                state.predicted_answer = AnswerExtractor.extract_answer(clean_response, strict=False)
            
            generation_time = time.time() - start_time
            avg_entropy = sum(entropy_values) / len(entropy_values) if entropy_values else 0.0
            
            # åˆ¤æ–­æ­£ç¡®æ€§
            is_correct = self._check_correctness(state.predicted_answer, ground_truth)
            
            # æ„å»ºç»“æœ
            result = {
                "sample_id": sample_id,
                "question": question,
                "ground_truth": ground_truth,
                "predicted_answer": state.predicted_answer,
                "correct": is_correct,
                "generation_time": generation_time,
                "tokens_used": state.tokens_used,
                "response": clean_response,
                "early_stopped": state.early_stopped,
                "halt_reason": state.halt_reason,
                "avg_entropy": avg_entropy,
                "entropy_history": entropy_values[:10],
                "stage_distribution": dict(Counter(stage_history))  # æ–°å¢ï¼šé˜¶æ®µåˆ†å¸ƒç»Ÿè®¡
            }
            
            # æ‰“å°ç»“æœ
            self._print_sample_result(result)
            return result
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._create_error_result(sample_id, question, ground_truth, str(e))
    
    def _check_correctness(self, predicted: Optional[str], ground_truth: str) -> bool:
        """æ£€æŸ¥ç­”æ¡ˆæ­£ç¡®æ€§"""
        if not predicted or not ground_truth:
            return False
        
        try:
            clean_pred = str(predicted).replace(',', '')
            clean_gt = str(ground_truth).replace(',', '')
            return float(clean_pred) == float(clean_gt)
        except ValueError:
            return str(predicted).strip() == str(ground_truth).strip()
    
    def _print_sample_result(self, result: Dict[str, Any]):
        """æ‰“å°å•ä¸ªæ ·æœ¬ç»“æœ"""
        status = "âœ… æ­£ç¡®" if result['correct'] else "âŒ é”™è¯¯"
        halt_info = f"| æ—©åœ: {result['halt_reason']}" if result['early_stopped'] else ""
        
        print(f"   {status} | é¢„æµ‹: {result['predicted_answer']} | "
              f"å®é™…: {result['ground_truth']} | "
              f"ç”¨æ—¶: {result['generation_time']:.1f}s | "
              f"Tokens: {result['tokens_used']} {halt_info}")
        
        if self.config['experiment'].get('verbose', False):
            preview = result['response'][:150].replace('\n', ' ')
            print(f"   å›ç­”é¢„è§ˆ: {preview}...")
    
    def _create_error_result(self, sample_id: int, question: str, ground_truth: str, error: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "sample_id": sample_id,
            "question": question,
            "ground_truth": ground_truth,
            "predicted_answer": None,
            "correct": False,
            "generation_time": 0,
            "tokens_used": 0,
            "response": f"Error: {error}",
            "error": error,
            "early_stopped": False,
            "halt_reason": None,
            "avg_entropy": 0.0,
            "entropy_history": []
        }
    
    def calculate_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            return {}
        
        total_samples = len(results)
        correct_samples = sum(1 for r in results if r['correct'])
        total_time = sum(r['generation_time'] for r in results)
        total_tokens = sum(r['tokens_used'] for r in results)
        early_stops = sum(1 for r in results if r.get('early_stopped', False))
        
        # ç»Ÿè®¡æ—©åœåŸå› 
        halt_reasons = Counter(r.get('halt_reason') for r in results if r.get('early_stopped', False))
        
        # å¹³å‡ç†µ
        avg_entropy = sum(r.get('avg_entropy', 0) for r in results) / total_samples
        
        # Tokenç»Ÿè®¡
        token_counts = [r['tokens_used'] for r in results]
        
        return {
            "total_samples": total_samples,
            "correct_samples": correct_samples,
            "accuracy": correct_samples / total_samples,
            "total_time": total_time,
            "avg_time_per_sample": total_time / total_samples,
            "total_tokens": total_tokens,
            "avg_tokens_per_sample": total_tokens / total_samples,
            "min_tokens": min(token_counts) if token_counts else 0,
            "max_tokens": max(token_counts) if token_counts else 0,
            "early_stops": early_stops,
            "early_stop_rate": early_stops / total_samples,
            "halt_reasons": dict(halt_reasons),
            "avg_entropy": avg_entropy
        }
    
    def print_statistics(self, stats: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š HALT-CoT å®éªŒç»Ÿè®¡ç»“æœ")
        print("=" * 60)
        print(f"ğŸ¤– æ¨¡å‹: {self.config['model_configs'][self.config['active_model']]['name']}")
        print(f"ğŸ“ æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"âœ… æ­£ç¡®æ ·æœ¬: {stats['correct_samples']}")
        print(f"ğŸ¯ å‡†ç¡®ç‡: {stats['accuracy']:.2%}")
        print(f"ğŸ›‘ æ—©åœç‡: {stats['early_stop_rate']:.2%} ({stats['early_stops']}/{stats['total_samples']})")
        print(f"â±ï¸  å¹³å‡ç”¨æ—¶: {stats['avg_time_per_sample']:.1f}ç§’/æ ·æœ¬")
        print(f"ğŸ’¬ å¹³å‡Token: {stats['avg_tokens_per_sample']:.1f}ä¸ª/æ ·æœ¬")
        print(f"ğŸ“Š TokenèŒƒå›´: {stats['min_tokens']} - {stats['max_tokens']}")
        print(f"ğŸ“‰ å¹³å‡ç†µ: {stats['avg_entropy']:.3f}")
        print(f"ğŸ• æ€»ç”¨æ—¶: {stats['total_time']//60:.0f}åˆ†{stats['total_time']%60:.0f}ç§’")
        
        if stats.get('halt_reasons'):
            print("\nğŸ” æ—©åœåŸå› åˆ†å¸ƒ:")
            for reason, count in stats['halt_reasons'].items():
                if reason:
                    print(f"   - {reason}: {count}æ¬¡")
        
        print("=" * 60)
    
    def save_results(self, results: List[Dict[str, Any]], stats: Dict[str, Any]) -> Optional[Path]:
        """ä¿å­˜å®éªŒç»“æœ"""
        if not self.config['experiment']['save_results']:
            print("âš ï¸  ç»“æœä¿å­˜å·²ç¦ç”¨")
            return None
    
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_key = self.config['active_model']
        sample_size = len(results)
    
        filename = f"halt_cot_{model_key}_{sample_size}samples_{timestamp}.json"
        results_file = RESULTS_DIR / filename
    
        # æ„å»ºç»Ÿè®¡æ‘˜è¦æ–‡æœ¬
        summary_text = f"""
{'=' * 60}
ğŸ“Š HALT-CoT å®éªŒç»Ÿè®¡ç»“æœ
{'=' * 60}
ğŸ¤– æ¨¡å‹: {self.config['model_configs'][model_key]['name']}
ğŸ“ æ€»æ ·æœ¬æ•°: {stats['total_samples']}
âœ… æ­£ç¡®æ ·æœ¬: {stats['correct_samples']}
ğŸ¯ å‡†ç¡®ç‡: {stats['accuracy']:.2%}
ğŸ›‘ æ—©åœç‡: {stats['early_stop_rate']:.2%} ({stats['early_stops']}/{stats['total_samples']})
â±ï¸  å¹³å‡ç”¨æ—¶: {stats['avg_time_per_sample']:.1f}ç§’/æ ·æœ¬
ğŸ’¬ å¹³å‡Token: {stats['avg_tokens_per_sample']:.1f}ä¸ª/æ ·æœ¬
ğŸ“Š TokenèŒƒå›´: {stats['min_tokens']} - {stats['max_tokens']}
ğŸ“‰ å¹³å‡ç†µ: {stats['avg_entropy']:.3f}
ğŸ• æ€»ç”¨æ—¶: {stats['total_time']//60:.0f}åˆ†{stats['total_time']%60:.0f}ç§’
"""
    
        # æ·»åŠ æ—©åœåŸå› åˆ†å¸ƒ
        if stats.get('halt_reasons'):
            summary_text += "\nğŸ” æ—©åœåŸå› åˆ†å¸ƒ:\n"
            for reason, count in stats['halt_reasons'].items():
                if reason:
                    summary_text += f"   - {reason}: {count}æ¬¡\n"
    
        summary_text += f"{'=' * 60}\n"
    
        save_data = {
            "experiment_info": {
            "timestamp": timestamp,
            "model": self.config['model_configs'][model_key]['name'],
            "model_key": model_key,
            "sample_size": sample_size,
            "config": self.config['experiment'],
            "early_stopping_config": self.config.get('early_stopping', {})
        },
        "statistics": stats,
        "summary": summary_text.strip(),  # æ·»åŠ æ–‡æœ¬æ‘˜è¦
        "results": results
        }
    
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
    
        # åŒæ—¶ä¿å­˜ä¸€ä¸ªçº¯æ–‡æœ¬çš„æ‘˜è¦æ–‡ä»¶
        summary_file = RESULTS_DIR / filename.replace('.json', '_summary.txt')
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_text)
    
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_file}")
        print(f"ğŸ“„ æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        return results_file

    
    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("ğŸ§ª å¼€å§‹HALT-CoTå¢å¼ºå®éªŒ")
        print("=" * 60)
        
        try:
            # 1. åŠ è½½æ•°æ®å’Œæ¨¡å‹
            test_data = self.load_test_data()
            tokenizer, model = self.load_model()
            
            # 2. è¿è¡Œå®éªŒ
            results = []
            experiment_start = time.time()
            
            for idx, item in enumerate(test_data):
                question = item['question']
                ground_truth = self.get_ground_truth(item)
                
                if ground_truth is None:
                    print(f"âš ï¸  æ ·æœ¬ {idx + 1} æ²¡æœ‰æœ‰æ•ˆç­”æ¡ˆï¼Œè·³è¿‡")
                    continue
                
                result = self.run_single_experiment(tokenizer, model, question, ground_truth, idx)
                results.append(result)
                
                # å®æ—¶è¿›åº¦æŠ¥å‘Š
                if (idx + 1) % 5 == 0:
                    self._print_progress_report(results, idx + 1, len(test_data))
            
            total_time = time.time() - experiment_start
            
            # 3. è®¡ç®—å’Œæ˜¾ç¤ºç»Ÿè®¡
            stats = self.calculate_statistics(results)
            stats['total_experiment_time'] = total_time
            
            self.print_statistics(stats)
            
            # 4. ä¿å­˜ç»“æœ
            results_file = self.save_results(results, stats)
            
            print(f"\nğŸ‰ å®éªŒå®Œæˆ!")
            if results_file:
                print(f"ğŸ“ ç»“æœæ–‡ä»¶: {results_file}")
            
            return results, stats
            
        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [], {}
    
    def _print_progress_report(self, results: List[Dict], current: int, total: int):
        """æ‰“å°è¿›åº¦æŠ¥å‘Š"""
        current_accuracy = sum(1 for r in results if r['correct']) / len(results)
        early_stop_rate = sum(1 for r in results if r.get('early_stopped', False)) / len(results)
        avg_tokens = sum(r['tokens_used'] for r in results) / len(results)
        
        print(f"ğŸ“Š è¿›åº¦: {current}/{total}, "
              f"å‡†ç¡®ç‡: {current_accuracy:.2%}, "
              f"æ—©åœç‡: {early_stop_rate:.2%}, "
              f"å¹³å‡tokens: {avg_tokens:.1f}")

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    """ä¸»å‡½æ•°"""
    try:
        runner = ExperimentRunner()
        results, stats = runner.run_experiment()
        
        if results:
            print(f"\nğŸ“ˆ å®éªŒæˆåŠŸå®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªæ ·æœ¬")
            print(f"ğŸ’¡ èåˆäº†ä¸¤ç§æ—©åœæ–¹æ³•ï¼š")
            print(f"   1. ç­”æ¡ˆä¸€è‡´æ€§æ£€æµ‹ï¼ˆLiu & Wang 2025ï¼‰")
            print(f"   2. ç†µåŸºæ—©åœï¼ˆLaaouach HALT-CoTï¼‰")
        else:
            print(f"\nğŸ’¥ å®éªŒå¤±è´¥æˆ–æ²¡æœ‰æœ‰æ•ˆç»“æœ")
            
    except KeyboardInterrupt:
        print(f"\nâŒ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ å®éªŒè¿è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()