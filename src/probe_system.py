"""æ™ºèƒ½æ¢é’ˆç³»ç»Ÿ"""
import torch
from typing import Optional

from .data_structures import CheckpointResult
from .answer_extractor import AnswerExtractor


class SmartProbeSystem:
    """æ™ºèƒ½æ¢é’ˆç³»ç»Ÿ - è¯†åˆ«æ¨ç†é˜¶æ®µå¹¶é€‰æ‹©æ€§æ¢æµ‹"""
    
    def __init__(self, model, tokenizer, debug: bool = False):
        """
        åˆå§‹åŒ–æ¢é’ˆç³»ç»Ÿ
        
        Args:
            model: è¯­è¨€æ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
        """
        self.model = model
        self.tokenizer = tokenizer
        self.debug = debug
        
        # æ¨ç†é˜¶æ®µæ ‡è®°è¯
        self.reasoning_markers = {
            'calculation': ['=', 'equals', 'total', 'sum', 'result'],
            'conclusion': ['therefore', 'thus', 'so', 'hence', 'finally'],
            'intermediate': ['step', 'first', 'next', 'then', 'now'],
            'answer_signal': ['answer is', 'answer:', '####', '\\boxed', 'final answer']
        }
    
    def identify_reasoning_stage(self, text: str) -> str:
        """
        è¯†åˆ«å½“å‰æ¨ç†é˜¶æ®µ
        
        Args:
            text: å½“å‰ç”Ÿæˆçš„æ–‡æœ¬
            
        Returns:
            æ¨ç†é˜¶æ®µæ ‡è¯†
        """
        text_lower = text.lower()
        last_200_chars = text_lower[-200:]
        
        # ä¼˜å…ˆçº§ï¼šç­”æ¡ˆä¿¡å· > ç»“è®º > è®¡ç®— > ä¸­é—´æ­¥éª¤
        for marker in self.reasoning_markers['answer_signal']:
            if marker in last_200_chars:
                return 'answer_signal'
        
        for marker in self.reasoning_markers['conclusion']:
            if marker in last_200_chars:
                return 'conclusion'
        
        for marker in self.reasoning_markers['calculation']:
            if marker in last_200_chars:
                return 'calculation'
        
        for marker in self.reasoning_markers['intermediate']:
            if marker in last_200_chars:
                return 'intermediate'
        
        return 'unknown'
    
    def should_probe_at_stage(self, stage: str) -> bool:
        """
        åˆ¤æ–­è¯¥é˜¶æ®µæ˜¯å¦åº”è¯¥æ¢æµ‹
        
        Args:
            stage: æ¨ç†é˜¶æ®µ
            
        Returns:
            æ˜¯å¦åº”è¯¥æ¢æµ‹
        """
        if stage == 'answer_signal':
            return True
        if stage == 'conclusion':
            return True
        if stage == 'calculation':
            return True
        if stage == 'intermediate':
            return False
        return False
    
    def detect_answer_in_context(self, text: str) -> Optional[str]:
        """
        ç›´æ¥ä»ä¸Šä¸‹æ–‡æ£€æµ‹ç­”æ¡ˆï¼ˆæ— éœ€æ¢é’ˆï¼‰
        
        Args:
            text: å½“å‰æ–‡æœ¬
            
        Returns:
            æ£€æµ‹åˆ°çš„ç­”æ¡ˆ
        """
        if '####' in text:
            import re
            match = re.search(r'####\s*(-?\d+(?:,\d+)*(?:\.\d+)?)', text)
            if match:
                return match.group(1).replace(',', '')
        
        if '\\boxed{' in text:
            import re
            match = re.search(r'\\boxed\{(-?\d+(?:,\d+)*(?:\.\d+)?)\}', text)
            if match:
                return match.group(1).replace(',', '')
        
        return None
    
    def create_probe_prompt(self, stage: str) -> str:
        """
        æ ¹æ®æ¨ç†é˜¶æ®µåˆ›å»ºåˆé€‚çš„æ¢é’ˆæç¤º
        
        Args:
            stage: æ¨ç†é˜¶æ®µ
            
        Returns:
            æ¢é’ˆæç¤ºæ–‡æœ¬
        """
        prompts = {
            'answer_signal': "\n#### ",
            'conclusion': "\n\nTherefore, the final answer is: ",
            'calculation': "\n\nThe result of this calculation is: ",
            'intermediate': "\n\nThe current value is: "
        }
        return prompts.get(stage, "\n#### ")
    
    def probe_answer(
        self, 
        full_sequence_ids: torch.Tensor, 
        current_text: str,
        stage: Optional[str] = None
    ) -> CheckpointResult:
        """
        æ™ºèƒ½æ¢é’ˆ - æ ¹æ®æ¨ç†é˜¶æ®µè°ƒæ•´ç­–ç•¥
        
        Args:
            full_sequence_ids: å®Œæ•´çš„tokenåºåˆ—
            current_text: å½“å‰æ–‡æœ¬
            stage: æ¨ç†é˜¶æ®µï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ£€æŸ¥ç‚¹ç»“æœ
        """
        # 1. å…ˆå°è¯•ç›´æ¥ä»ä¸Šä¸‹æ–‡æå–
        context_answer = self.detect_answer_in_context(current_text)
        if context_answer:
            if self.debug:
                print(f"      âœ“ ç›´æ¥æå–: {context_answer}")
            return CheckpointResult(
                answer=context_answer,
                entropy=0.1,
                confidence=0.95
            )
        
        # 2. è¯†åˆ«æ¨ç†é˜¶æ®µ
        if stage is None:
            stage = self.identify_reasoning_stage(current_text)
        
        if self.debug:
            print(f"      ğŸ¯ æ¨ç†é˜¶æ®µ: {stage}")
        
        # 3. æ ¹æ®é˜¶æ®µå†³å®šæ˜¯å¦æ¢æµ‹
        if not self.should_probe_at_stage(stage):
            return CheckpointResult(
                should_halt=False,
                answer=None,
                entropy=100.0
            )
        
        # 4. æ‰§è¡Œæ¢é’ˆ
        try:
            probe_text = self.create_probe_prompt(stage)
            probe_tokens = self.tokenizer.encode(
                probe_text, 
                return_tensors='pt', 
                add_special_tokens=False
            ).to(self.model.device)
            
            probe_input_ids = torch.cat([full_sequence_ids, probe_tokens], dim=-1)
            
            max_new_tokens = 30 if stage == 'answer_signal' else 20
            
            with torch.no_grad():
                gen_output = self.model.generate(
                    probe_input_ids,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    output_scores=True,
                    return_dict_in_generate=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # 5. è§£æç»“æœ
            if gen_output.sequences.shape[1] > probe_input_ids.shape[1]:
                answer_tokens = gen_output.sequences[0][probe_input_ids.shape[1]:]
                probe_response = self.tokenizer.decode(
                    answer_tokens, 
                    skip_special_tokens=True
                ).strip()
                
                if self.debug:
                    print(f"      ğŸ“ æ¢é’ˆå“åº”: '{probe_response.replace(chr(10), '  ')}'")
                
                answer = AnswerExtractor.extract_answer(probe_response)
                
                if not answer:
                    answer = self._extract_number_fallback(probe_response)
                
                if self.debug:
                    print(f"      ğŸ”¢ æå–ç­”æ¡ˆ: '{answer}'")
                
                # è®¡ç®—ç†µ
                entropy = 100.0
                confidence = 0.0
                
                if gen_output.scores and len(gen_output.scores) > 0:
                    first_token_logits = gen_output.scores[0][0]
                    probs = torch.softmax(first_token_logits, dim=-1)
                    log_probs = torch.log_softmax(first_token_logits, dim=-1)
                    entropy = -torch.sum(probs * log_probs).item()
                    confidence = 1.0 / (1.0 + entropy)
                
                return CheckpointResult(
                    should_halt=False,
                    answer=answer,
                    entropy=entropy,
                    confidence=confidence
                )
        
        except Exception as e:
            if self.debug:
                print(f"      âŒ æ¢é’ˆé”™è¯¯: {e}")
        
        return CheckpointResult(entropy=100.0)
    
    def _extract_number_fallback(self, text: str) -> Optional[str]:
        """åå¤‡æ•°å­—æå–æ–¹æ¡ˆ"""
        import re
        match = re.search(r'-?\d+(?:,\d{3})*(?:\.\d+)?', text)
        if match:
            return match.group(0).replace(',', '')
        
        match = re.search(r'-?\d+\.?\d*', text)
        if match:
            return match.group(0)
        
        return None
