"""å®éªŒè¿è¡Œå™¨ - ç²¾ç®€ç‰ˆ"""
import json
import time
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM

from .config import ConfigManager
from .experiment_manager import ExperimentManager
from .statistics_calculator import StatisticsCalculator
from .result_saver import ResultSaver


class ExperimentRunner:
    """
    ç²¾ç®€çš„å®éªŒè¿è¡Œå™¨
    åªè´Ÿè´£åè°ƒå„ä¸ªç»„ä»¶ï¼Œä¸å¤„ç†å…·ä½“é€»è¾‘ [[0]](#__0)
    """
    
    def __init__(self, base_dir: Path = None):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨
        
        Args:
            base_dir: åŸºç¡€ç›®å½•è·¯å¾„
        """
        if base_dir is None:
            base_dir = Path("/root/autodl-tmp")
        
        self.base_dir = base_dir
        self.config_dir = base_dir / "config"
        self.data_dir = base_dir / "data"
        self.results_dir = base_dir / "results"
        
        self.results_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®
        config_file = self.config_dir / "config.json"
        self.config = ConfigManager.load_config(config_file)
        
        # åˆå§‹åŒ–è¾…åŠ©ç±»
        self.statistics_calculator = StatisticsCalculator()
        self.result_saver = ResultSaver(self.results_dir, self.config)
        
        print(f"ğŸš€ å®éªŒè¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_test_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        test_file = Path(self.config['paths']['test_data'])
        if not test_file.is_absolute():
            test_file = self.base_dir / test_file
        
        if not test_file.exists():
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_file}")
        
        with open(test_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        sample_size = self.config['experiment']['sample_size']
        if len(data) > sample_size:
            data = data[:sample_size]
        
        print(f"âœ… å·²åŠ è½½ {len(data)} æ¡æµ‹è¯•æ•°æ®")
        return data
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        model_key = self.config['active_model']
        model_name = self.config['model_configs'][model_key]['name']
        
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")
        return tokenizer, model
    
    def get_ground_truth(self, item: Dict[str, Any]) -> Optional[str]:
        """è·å–æ ‡å‡†ç­”æ¡ˆ"""
        from .answer_extractor import AnswerExtractor
        
        if 'numerical_answer' in item and item['numerical_answer']:
            return str(item['numerical_answer'])
        
        if 'answer' in item:
            return AnswerExtractor.extract_answer(item['answer'])
        
        return None
    
    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("ğŸ§ª å¼€å§‹HALT-CoTå¢å¼ºå®éªŒ")
        print("=" * 60)
        
        try:
            # åŠ è½½æ•°æ®å’Œæ¨¡å‹
            test_data = self.load_test_data()
            tokenizer, model = self.load_model()
            
            # åˆ›å»ºå®éªŒç®¡ç†å™¨
            experiment_manager = ExperimentManager(self.config, model, tokenizer)
            
            # è¿è¡Œå®éªŒ
            results = []
            experiment_start = time.time()
            
            for idx, item in enumerate(test_data):
                question = item['question']
                ground_truth = self.get_ground_truth(item)
                
                if ground_truth is None:
                    print(f"âš ï¸  æ ·æœ¬ {idx + 1} æ²¡æœ‰æœ‰æ•ˆç­”æ¡ˆï¼Œè·³è¿‡")
                    continue
                
                result = experiment_manager.run_single_sample(question, ground_truth, idx)
                results.append(result)
                
                if (idx + 1) % 5 == 0:
                    self._print_progress_report(results, idx + 1, len(test_data))
            
            total_time = time.time() - experiment_start
            
            # è®¡ç®—ç»Ÿè®¡
            stats = self.statistics_calculator.calculate(results)
            stats['total_experiment_time'] = total_time
            
            self.statistics_calculator.print_statistics(stats, self.config)
            
            # ä¿å­˜ç»“æœ
            results_file = self.result_saver.save(results, stats)
            
            print(f"\nğŸ‰ å®éªŒå®Œæˆ!")
            if results_file:
                print(f"ğŸ“ ç»“æœæ–‡ä»¶: {results_file}")
            
            return results, stats
            
        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [], {}
    
    def _print_progress_report(self, results: List[Dict], current: int, total: int):
        """æ‰“å°è¿›åº¦æŠ¥å‘Š"""
        current_accuracy = sum(1 for r in results if r['correct']) / len(results)
        early_stop_rate = sum(1 for r in results if r.get('early_stopped', False)) / len(results)
        avg_tokens = sum(r['tokens_used'] for r in results) / len(results)
        
        print(f"ğŸ“Š è¿›åº¦: {current}/{total}, "
              f"å‡†ç¡®ç‡: {current_accuracy:.2%}, "
              f"æ—©åœç‡: {early_stop_rate:.2%}, "
              f"å¹³å‡tokens: {avg_tokens:.1f}")
