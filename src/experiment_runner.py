"""å®éªŒè¿è¡Œå™¨"""
import torch
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM

from .config import ConfigManager
from .data_structures import GenerationState
from .answer_extractor import AnswerExtractor
from .early_stopping import SmartHaltDecisionMaker
from .probe_system import SmartProbeSystem
from .generation_manager import GenerationManager


class ExperimentRunner:
    """ä½¿ç”¨æ™ºèƒ½æ¢é’ˆçš„å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, base_dir: Path = None):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨
        
        Args:
            base_dir: åŸºç¡€ç›®å½•è·¯å¾„
        """
        if base_dir is None:
            base_dir = Path("/root/autodl-tmp")
        
        self.base_dir = base_dir
        self.config_dir = base_dir / "config"
        self.data_dir = base_dir / "data"
        self.results_dir = base_dir / "results"
        
        self.results_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®
        config_file = self.config_dir / "config.json"
        self.config = ConfigManager.load_config(config_file)
        
        # åˆå§‹åŒ–ç®¡ç†å™¨
        self.generation_manager = GenerationManager(self.config)
        self.halt_decision_maker = SmartHaltDecisionMaker(self.config)
        
        self.debug_mode = self.config.get('experiment', {}).get('debug_probe', False)
        
        print(f"ğŸ”§ æ™ºèƒ½æ¢é’ˆé…ç½®: ç­”æ¡ˆä¸€è‡´æ€§={self.halt_decision_maker.use_consistency}, "
              f"ç†µæ£€æµ‹={self.halt_decision_maker.use_entropy}, "
              f"è°ƒè¯•æ¨¡å¼={self.debug_mode}")
    
    def load_test_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        test_file = Path(self.config['paths']['test_data'])
        if not test_file.is_absolute():
            test_file = self.base_dir / test_file
        
        if not test_file.exists():
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {test_file}")
        
        with open(test_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        sample_size = self.config['experiment']['sample_size']
        if len(data) > sample_size:
            data = data[:sample_size]
        
        print(f"âœ… å·²åŠ è½½ {len(data)} æ¡æµ‹è¯•æ•°æ®")
        return data
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        model_key = self.config['active_model']
        model_name = self.config['model_configs'][model_key]['name']
        
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")
        return tokenizer, model
    
    def get_ground_truth(self, item: Dict[str, Any]) -> Optional[str]:
        """è·å–æ ‡å‡†ç­”æ¡ˆ"""
        if 'numerical_answer' in item and item['numerical_answer']:
            return str(item['numerical_answer'])
        
        if 'answer' in item:
            return AnswerExtractor.extract_answer(item['answer'])
        
        return None
    
    def run_single_experiment(
        self, 
        tokenizer, 
        model, 
        question: str, 
        ground_truth: str, 
        sample_id: int
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        prompt = f"""Question: {question}

Please solve this step by step and provide your final answer.

Answer:"""
        
        print(f"\nğŸ“ æ ·æœ¬ {sample_id + 1}: {question[:80]}...")
        start_time = time.time()
        
        # åˆå§‹åŒ–
        state = GenerationState()
        probe_system = SmartProbeSystem(model, tokenizer, debug=self.debug_mode)
        self.halt_decision_maker.reset()
        
        # å‡†å¤‡è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        input_ids = inputs['input_ids'].to(model.device)
        attention_mask = inputs['attention_mask'].to(model.device)
        
        state.full_sequence_ids = input_ids.clone()
        
        # ç”Ÿæˆå‚æ•°
        exp_config = self.config.get('experiment', {})
        max_new_tokens = exp_config.get('max_new_tokens', 512)
        temperature = exp_config.get('temperature', 0.7)
        do_sample = exp_config.get('do_sample', False)
        
        try:
            # ä¸»ç”Ÿæˆå¾ªç¯
            past_key_values = None
            current_input_ids = input_ids
            entropy_values = []
            stage_history = []
            
            while state.tokens_used < max_new_tokens and not state.early_stopped:
                # æ¨¡å‹å‰å‘ä¼ æ’­
                with torch.no_grad():
                    outputs = model(
                        input_ids=current_input_ids,
                        past_key_values=past_key_values,
                        use_cache=True,
                        attention_mask=attention_mask if past_key_values is None else None
                    )
                
                # é€‰æ‹©ä¸‹ä¸€ä¸ªtoken
                next_token_logits = outputs.logits[:, -1, :]
                past_key_values = outputs.past_key_values
                
                if do_sample:
                    probs = torch.softmax(next_token_logits / temperature, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)
                
                # æ›´æ–°çŠ¶æ€
                state.full_sequence_ids = torch.cat([state.full_sequence_ids, next_token], dim=-1)
                state.tokens_used += 1
                current_input_ids = next_token
                
                # è§£ç æ–°token
                new_token_id = next_token.item()
                new_text = tokenizer.decode([new_token_id], skip_special_tokens=True)
                state.full_response += new_text
                
                # æ£€æŸ¥è‡ªç„¶åœæ­¢
                should_stop, stop_reason = self.generation_manager.should_stop_naturally(
                    state.full_response, new_token_id, tokenizer
                )
                if should_stop:
                    print(f"   ğŸ›‘ è‡ªç„¶åœæ­¢: {stop_reason}")
                    break
                
                # æ™ºèƒ½æ£€æŸ¥ç‚¹
                min_tokens = self.config.get('early_stopping', {}).get('min_tokens_before_check', 100)
                
                if state.tokens_used >= min_tokens:
                    current_stage = probe_system.identify_reasoning_stage(state.full_response)
                    stage_history.append(current_stage)
                    
                    cooldown = self.config.get('early_stopping', {}).get('cooldown_tokens', 40)
                    
                    if self.halt_decision_maker.should_check_now(
                        state.full_response, 
                        state.tokens_used, 
                        current_stage,
                        cooldown
                    ):
                        probe_result = probe_system.probe_answer(
                            state.full_sequence_ids,
                            state.full_response,
                            current_stage
                        )
                        
                        self.halt_decision_maker.update_check_state(state.tokens_used, current_stage)
                        
                        if probe_result.answer:
                            clean_context = state.full_response[-100:].replace('\n', 'â')
                            print(f"   ğŸ” [æ£€æŸ¥ç‚¹@{current_stage}] Tokens: {state.tokens_used}")
                            print(f"      ğŸ“„ ä¸Šä¸‹æ–‡: ...{clean_context}")
                            print(f"      ğŸ§ª æ¢é’ˆ: '{probe_result.answer}' | ç†µ: {probe_result.entropy:.4f}")
                            
                            entropy_values.append(probe_result.entropy)
                        
                        decision = self.halt_decision_maker.make_decision(probe_result, current_stage)
                        
                        if decision.should_halt:
                            state.early_stopped = True
                            state.halt_reason = decision.halt_reason
                            state.predicted_answer = decision.answer
                            print(f"   ğŸ›‘ [æ—©åœ] {decision.halt_reason} | ç­”æ¡ˆ: {decision.answer}")
                            break
            
            # æ¸…ç†å“åº”
            clean_response = state.full_response
            for stop_word in self.generation_manager.stop_words:
                if stop_word in clean_response:
                    clean_response = clean_response.split(stop_word)[0].strip()
            
            # æå–æœ€ç»ˆç­”æ¡ˆ
            if not state.predicted_answer:
                state.predicted_answer = AnswerExtractor.extract_answer(clean_response, strict=False)
            
            generation_time = time.time() - start_time
            avg_entropy = sum(entropy_values) / len(entropy_values) if entropy_values else 0.0
            
            # åˆ¤æ–­æ­£ç¡®æ€§
            is_correct = self._check_correctness(state.predicted_answer, ground_truth)
            
            from collections import Counter
            
            # æ„å»ºç»“æœ
            result = {
                "sample_id": sample_id,
                "question": question,
                "ground_truth": ground_truth,
                "predicted_answer": state.predicted_answer,
                "correct": is_correct,
                "generation_time": generation_time,
                "tokens_used": state.tokens_used,
                "response": clean_response,
                "early_stopped": state.early_stopped,
                "halt_reason": state.halt_reason,
                "avg_entropy": avg_entropy,
                "entropy_history": entropy_values[:10],
                "stage_distribution": dict(Counter(stage_history))
            }
            
            self._print_sample_result(result)
            return result
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._create_error_result(sample_id, question, ground_truth, str(e))
    
    def _check_correctness(self, predicted: Optional[str], ground_truth: str) -> bool:
        """æ£€æŸ¥ç­”æ¡ˆæ­£ç¡®æ€§"""
        if not predicted or not ground_truth:
            return False
        
        try:
            clean_pred = str(predicted).replace(',', '')
            clean_gt = str(ground_truth).replace(',', '')
            return float(clean_pred) == float(clean_gt)
        except ValueError:
            return str(predicted).strip() == str(ground_truth).strip()
    
    def _print_sample_result(self, result: Dict[str, Any]):
        """æ‰“å°å•ä¸ªæ ·æœ¬ç»“æœ"""
        status = "âœ… æ­£ç¡®" if result['correct'] else "âŒ é”™è¯¯"
        halt_info = f"| æ—©åœ: {result['halt_reason']}" if result['early_stopped'] else ""
        
        print(f"   {status} | é¢„æµ‹: {result['predicted_answer']} | "
              f"å®é™…: {result['ground_truth']} | "
              f"ç”¨æ—¶: {result['generation_time']:.1f}s | "
              f"Tokens: {result['tokens_used']} {halt_info}")
        
        if self.config['experiment'].get('verbose', False):
            preview = result['response'][:150].replace('\n', ' ')
            print(f"   å›ç­”é¢„è§ˆ: {preview}...")
    
    def _create_error_result(self, sample_id: int, question: str, 
                            ground_truth: str, error: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            "sample_id": sample_id,
            "question": question,
            "ground_truth": ground_truth,
            "predicted_answer": None,
            "correct": False,
            "generation_time": 0,
            "tokens_used": 0,
            "response": f"Error: {error}",
            "error": error,
            "early_stopped": False,
            "halt_reason": None,
            "avg_entropy": 0.0,
            "entropy_history": []
        }
    
    def calculate_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            return {}
        
        from collections import Counter
        
        total_samples = len(results)
        correct_samples = sum(1 for r in results if r['correct'])
        total_time = sum(r['generation_time'] for r in results)
        total_tokens = sum(r['tokens_used'] for r in results)
        early_stops = sum(1 for r in results if r.get('early_stopped', False))
        
        halt_reasons = Counter(r.get('halt_reason') for r in results if r.get('early_stopped', False))
        avg_entropy = sum(r.get('avg_entropy', 0) for r in results) / total_samples
        token_counts = [r['tokens_used'] for r in results]
        
        return {
            "total_samples": total_samples,
            "correct_samples": correct_samples,
            "accuracy": correct_samples / total_samples,
            "total_time": total_time,
            "avg_time_per_sample": total_time / total_samples,
            "total_tokens": total_tokens,
            "avg_tokens_per_sample": total_tokens / total_samples,
            "min_tokens": min(token_counts) if token_counts else 0,
            "max_tokens": max(token_counts) if token_counts else 0,
            "early_stops": early_stops,
            "early_stop_rate": early_stops / total_samples,
            "halt_reasons": dict(halt_reasons),
            "avg_entropy": avg_entropy
        }
    
    def print_statistics(self, stats: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("ğŸ“Š HALT-CoT å®éªŒç»Ÿè®¡ç»“æœ")
        print("=" * 60)
        print(f"ğŸ¤– æ¨¡å‹: {self.config['model_configs'][self.config['active_model']]['name']}")
        print(f"ğŸ“ æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"âœ… æ­£ç¡®æ ·æœ¬: {stats['correct_samples']}")
        print(f"ğŸ¯ å‡†ç¡®ç‡: {stats['accuracy']:.2%}")
        print(f"ğŸ›‘ æ—©åœç‡: {stats['early_stop_rate']:.2%} ({stats['early_stops']}/{stats['total_samples']})")
        print(f"â±ï¸  å¹³å‡ç”¨æ—¶: {stats['avg_time_per_sample']:.1f}ç§’/æ ·æœ¬")
        print(f"ğŸ’¬ å¹³å‡Token: {stats['avg_tokens_per_sample']:.1f}ä¸ª/æ ·æœ¬")
        print(f"ğŸ“Š TokenèŒƒå›´: {stats['min_tokens']} - {stats['max_tokens']}")
        print(f"ğŸ“‰ å¹³å‡ç†µ: {stats['avg_entropy']:.3f}")
        print(f"ğŸ• æ€»ç”¨æ—¶: {stats['total_time']//60:.0f}åˆ†{stats['total_time']%60:.0f}ç§’")
        
        if stats.get('halt_reasons'):
            print("\nğŸ” æ—©åœåŸå› åˆ†å¸ƒ:")
            for reason, count in stats['halt_reasons'].items():
                if reason:
                    print(f"   - {reason}: {count}æ¬¡")
        
        print("=" * 60)
    
    def save_results(self, results: List[Dict[str, Any]], 
                    stats: Dict[str, Any]) -> Optional[Path]:
        """ä¿å­˜å®éªŒç»“æœ"""
        if not self.config['experiment']['save_results']:
            print("âš ï¸  ç»“æœä¿å­˜å·²ç¦ç”¨")
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_key = self.config['active_model']
        sample_size = len(results)
        
        filename = f"halt_cot_{model_key}_{sample_size}samples_{timestamp}.json"
        results_file = self.results_dir / filename
        
        # æ„å»ºç»Ÿè®¡æ‘˜è¦
        summary_text = f"""
{'=' * 60}
ğŸ“Š HALT-CoT å®éªŒç»Ÿè®¡ç»“æœ
{'=' * 60}
ğŸ¤– æ¨¡å‹: {self.config['model_configs'][model_key]['name']}
ğŸ“ æ€»æ ·æœ¬æ•°: {stats['total_samples']}
âœ… æ­£ç¡®æ ·æœ¬: {stats['correct_samples']}
ğŸ¯ å‡†ç¡®ç‡: {stats['accuracy']:.2%}
ğŸ›‘ æ—©åœç‡: {stats['early_stop_rate']:.2%} ({stats['early_stops']}/{stats['total_samples']})
â±ï¸  å¹³å‡ç”¨æ—¶: {stats['avg_time_per_sample']:.1f}ç§’/æ ·æœ¬
ğŸ’¬ å¹³å‡Token: {stats['avg_tokens_per_sample']:.1f}ä¸ª/æ ·æœ¬
ğŸ“Š TokenèŒƒå›´: {stats['min_tokens']} - {stats['max_tokens']}
ğŸ“‰ å¹³å‡ç†µ: {stats['avg_entropy']:.3f}
ğŸ• æ€»ç”¨æ—¶: {stats['total_time']//60:.0f}åˆ†{stats['total_time']%60:.0f}ç§’
"""
        
        if stats.get('halt_reasons'):
            summary_text += "\nğŸ” æ—©åœåŸå› åˆ†å¸ƒ:\n"
            for reason, count in stats['halt_reasons'].items():
                if reason:
                    summary_text += f"   - {reason}: {count}æ¬¡\n"
        
        summary_text += f"{'=' * 60}\n"
        
        save_data = {
            "experiment_info": {
                "timestamp": timestamp,
                "model": self.config['model_configs'][model_key]['name'],
                "model_key": model_key,
                "sample_size": sample_size,
                "config": self.config['experiment'],
                "early_stopping_config": self.config.get('early_stopping', {})
            },
            "statistics": stats,
            "summary": summary_text.strip(),
            "results": results
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜çº¯æ–‡æœ¬æ‘˜è¦
        summary_file = self.results_dir / filename.replace('.json', '_summary.txt')
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_file}")
        print(f"ğŸ“„ æ‘˜è¦å·²ä¿å­˜: {summary_file}")
        return results_file
    
    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("ğŸ§ª å¼€å§‹HALT-CoTå¢å¼ºå®éªŒ")
        print("=" * 60)
        
        try:
            # åŠ è½½æ•°æ®å’Œæ¨¡å‹
            test_data = self.load_test_data()
            tokenizer, model = self.load_model()
            
            # è¿è¡Œå®éªŒ
            results = []
            experiment_start = time.time()
            
            for idx, item in enumerate(test_data):
                question = item['question']
                ground_truth = self.get_ground_truth(item)
                
                if ground_truth is None:
                    print(f"âš ï¸  æ ·æœ¬ {idx + 1} æ²¡æœ‰æœ‰æ•ˆç­”æ¡ˆï¼Œè·³è¿‡")
                    continue
                
                result = self.run_single_experiment(tokenizer, model, question, ground_truth, idx)
                results.append(result)
                
                if (idx + 1) % 5 == 0:
                    self._print_progress_report(results, idx + 1, len(test_data))
            
            total_time = time.time() - experiment_start
            
            # è®¡ç®—ç»Ÿè®¡
            stats = self.calculate_statistics(results)
            stats['total_experiment_time'] = total_time
            
            self.print_statistics(stats)
            
            # ä¿å­˜ç»“æœ
            results_file = self.save_results(results, stats)
            
            print(f"\nğŸ‰ å®éªŒå®Œæˆ!")
            if results_file:
                print(f"ğŸ“ ç»“æœæ–‡ä»¶: {results_file}")
            
            return results, stats
            
        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [], {}
    
    def _print_progress_report(self, results: List[Dict], current: int, total: int):
        """æ‰“å°è¿›åº¦æŠ¥å‘Š"""
        current_accuracy = sum(1 for r in results if r['correct']) / len(results)
        early_stop_rate = sum(1 for r in results if r.get('early_stopped', False)) / len(results)
        avg_tokens = sum(r['tokens_used'] for r in results) / len(results)
        
        print(f"ğŸ“Š è¿›åº¦: {current}/{total}, "
              f"å‡†ç¡®ç‡: {current_accuracy:.2%}, "
              f"æ—©åœç‡: {early_stop_rate:.2%}, "
              f"å¹³å‡tokens: {avg_tokens:.1f}")