#!/usr/bin/env python3
"""
å¢å¼ºå‹HALT-CoTå®éªŒè¿è¡Œå™¨ - ç†µå€¼åˆ†æä¸å¯è§†åŒ–ç‰ˆ (ä¿®å¤ç‰ˆv3)
- ä¿®å¤è¿‡åº¦æˆªæ–­é—®é¢˜
- åœ¨æ‰€æœ‰æ¢æµ‹ç‚¹æ ‡æ³¨ç­”æ¡ˆ
- ä¼˜åŒ–æ ‡æ³¨å¸ƒå±€é¿å…é‡å 
"""

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import time
import re
import os
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from collections import Counter
from dataclasses import dataclass, field, asdict

# å°è¯•å¯¼å…¥ç»˜å›¾åº“
try:
    import matplotlib.pyplot as plt
    import matplotlib.colors as mcolors
    import seaborn as sns
    from matplotlib.gridspec import GridSpec
    HAS_PLOT = True
except ImportError:
    HAS_PLOT = False
    print("âš ï¸ æœªæ£€æµ‹åˆ° matplotlib/seabornï¼Œå°†è·³è¿‡ç»˜å›¾åŠŸèƒ½ã€‚å»ºè®® pip install matplotlib seaborn")

# ============================================================================
# é…ç½®å’Œè·¯å¾„å¸¸é‡
# ============================================================================
BASE_DIR = Path("/root/autodl-tmp")
CONFIG_DIR = BASE_DIR / "config_entropy"
DATA_DIR = BASE_DIR / "data"
RESULTS_DIR = BASE_DIR / "results_entropy"

RESULTS_DIR.mkdir(parents=True, exist_ok=True)
(RESULTS_DIR / "plots").mkdir(exist_ok=True)

# ============================================================================
# æ•°æ®ç»“æ„
# ============================================================================
@dataclass
class GenerationState:
    tokens_used: int = 0
    full_response: str = ""
    full_sequence_ids: Optional[torch.Tensor] = None
    early_stopped: bool = False
    halt_reason: Optional[str] = None
    predicted_answer: Optional[str] = None

@dataclass 
class CheckpointResult:
    should_halt: bool = False
    halt_reason: Optional[str] = None
    answer: Optional[str] = None
    entropy: float = 100.0
    confidence: float = 0.0

@dataclass
class ProbeRecord:
    """è®°å½•å•æ¬¡æ¢æµ‹çš„è¯¦ç»†ä¿¡æ¯"""
    step: int
    stage: str
    answer: Optional[str]
    entropy: float
    text_segment: str = ""
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿JSONåºåˆ—åŒ–"""
        return {
            'step': self.step,
            'stage': self.stage,
            'answer': self.answer,
            'entropy': self.entropy,
            'text_segment': self.text_segment
        }
    
    @classmethod
    def from_dict(cls, data: dict):
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            step=data.get('step', 0),
            stage=data.get('stage', 'unknown'),
            answer=data.get('answer'),
            entropy=data.get('entropy', 0.0),
            text_segment=data.get('text_segment', '')
        )

# ============================================================================
# æ–‡æœ¬æ¸…ç†å·¥å…·
# ============================================================================
class TextCleaner:
    """ç”¨äºæ¸…ç†å’Œæˆªæ–­å¼‚å¸¸è¾“å‡º"""
    
    # éœ€è¦æˆªæ–­çš„å¼‚å¸¸æ¨¡å¼ï¼ˆç§»é™¤äº† ### å› ä¸ºå®ƒæ˜¯GSM8Kçš„ç­”æ¡ˆæ ‡è®°ï¼‰
    STOP_PATTERNS = [
        "You are an AI assistant",
        "You are a helpful assistant",
        "I am an AI",
        "As an AI",
        "Human:",
        "Assistant:",
        "User:",
        "<|im_start|>",
        "<|im_end|>",
    ]
    
    @staticmethod
    def clean_response(text: str, verbose: bool = False) -> str:
        """æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤å¼‚å¸¸è¾“å‡º"""
        if not text:
            return text
        
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå¼‚å¸¸æ¨¡å¼çš„ä½ç½®
        min_pos = len(text)
        found_pattern = None
        
        for pattern in TextCleaner.STOP_PATTERNS:
            pos = text.find(pattern)
            if pos != -1 and pos < min_pos:
                min_pos = pos
                found_pattern = pattern
        
        # å¦‚æœæ‰¾åˆ°å¼‚å¸¸æ¨¡å¼ï¼Œæˆªæ–­åˆ°è¯¥ä½ç½®
        if found_pattern:
            text = text[:min_pos].strip()
            if verbose:
                print(f"   âš ï¸ Truncated at pattern: '{found_pattern}'")
        
        return text
    
    @staticmethod
    def extract_reasoning_part(text: str) -> str:
        """æå–æ¨ç†éƒ¨åˆ†ï¼Œç§»é™¤å‰åçš„æ— å…³å†…å®¹"""
        # ç§»é™¤å¼€å¤´çš„æç¤ºè¯
        text = re.sub(r'^(Question:|Answer:|Problem:|Solution:)\s*', '', text, flags=re.IGNORECASE)
        
        # æ¸…ç†å¼‚å¸¸è¾“å‡ºï¼ˆä½†ä¸æ‰“å°è­¦å‘Šï¼‰
        text = TextCleaner.clean_response(text, verbose=False)
        
        return text.strip()

# ============================================================================
# å¢å¼ºçš„ç­”æ¡ˆæå–å™¨
# ============================================================================
# ============================================================================
# å¢å¼ºçš„ç­”æ¡ˆæå–å™¨ (ä¿®å¤ç‰ˆ)
# ============================================================================
class AnswerExtractor:
    @staticmethod
    def extract_answer(text: str, strict: bool = False) -> Optional[str]:
        """å¢å¼ºç‰ˆç­”æ¡ˆæå–å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        if not text:
            return None
        
        # å…ˆæ¸…ç†æ–‡æœ¬
        text = TextCleaner.extract_reasoning_part(text)
        
        # 1. æ ‡å‡†æ ¼å¼åŒ¹é…ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        patterns = [
            # GSM8Kæ ‡å‡†æ ¼å¼
            (r'####\s*(-?\d+(?:,\d+)*(?:\.\d+)?)', 10),
            # LaTeX boxedæ ¼å¼
            (r'\\boxed\{(-?\d+(?:,\d+)*(?:\.\d+)?)\}', 9),
            # "Answer: X" æ ¼å¼ï¼ˆæ–°å¢ï¼‰
            (r'[Aa]nswer:\s*(-?\d+(?:,\d+)*(?:\.\d+)?)', 9),
            # "Therefore, the answer is X" æ ¼å¼
            (r'[Tt]herefore,?\s+(?:the\s+)?(?:answer|total|result)\s+(?:is|equals?)\s+\$?\s*(-?\d+(?:,\d+)*(?:\.\d+)?)', 8),
            # "The answer is X" æ ¼å¼
            (r'[Tt]he\s+(?:final\s+)?(?:answer|total|result)\s+(?:is|equals?)\s+\$?\s*(-?\d+(?:,\d+)*(?:\.\d+)?)', 7),
            # "John is X miles from home" æ ¼å¼ï¼ˆæ–°å¢ï¼‰
            (r'(?:is|are)\s+(-?\d+(?:,\d+)*(?:\.\d+)?)\s+(?:miles?|dollars?|units?)', 7),
            # "answer is X" æ ¼å¼
            (r'(?:answer|result|total)\s+(?:is|equals?|=)\s*\$?\s*(-?\d+(?:,\d+)*(?:\.\d+)?)', 6),
            # å¥å­ç»“å°¾çš„æ•°å­—ï¼ˆå¸¦å•ä½æˆ–æ ‡ç‚¹ï¼‰
            (r'(?:is|equals?|=)\s+(-?\d+(?:,\d+)*(?:\.\d+)?)\s*(?:downloads?|dollars?|miles?|units?|\.|$)', 5),
        ]
        
        best_answer = None
        best_priority = -1
        best_position = -1
        
        for pattern, priority in patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            if matches:
                # å–æœ€åä¸€ä¸ªåŒ¹é…ï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
                match = matches[-1]
                answer = match.group(1).replace(',', '').strip()
                try:
                    val = float(answer)
                    # æ’é™¤æ˜æ˜¾é”™è¯¯çš„å€¼
                    if val < 0 or val > 1e10:
                        continue
                    
                    position = match.start()
                    # ä¼˜å…ˆçº§ç›¸åŒæ—¶ï¼Œé€‰æ‹©ä½ç½®æ›´é åçš„
                    if priority > best_priority or (priority == best_priority and position > best_position):
                        best_answer = answer
                        best_priority = priority
                        best_position = position
                except ValueError:
                    continue
        
        if best_answer:
            return best_answer
        
        # 2. å°è¯•ä»æœ€åå‡ è¡Œæå–
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        if lines:
            # æ£€æŸ¥æœ€å5è¡Œ
            for line in reversed(lines[-5:]):
                # è·³è¿‡è¿‡é•¿çš„è¯´æ˜æ€§æ–‡æœ¬
                if len(line) > 150:
                    continue
                
                # ä¼˜å…ˆæŸ¥æ‰¾åŒ…å«"Answer:"çš„è¡Œ
                if re.search(r'\bAnswer:', line, re.IGNORECASE):
                    numbers = re.findall(r'-?\d+(?:,\d+)*(?:\.\d+)?', line)
                    if numbers:
                        answer = numbers[0].replace(',', '')  # å–ç¬¬ä¸€ä¸ªæ•°å­—
                        try:
                            val = float(answer)
                            if 0 <= val < 1e10:
                                return answer
                        except ValueError:
                            continue
                
                # æŸ¥æ‰¾åŒ…å«"æ€»è®¡"ã€"ç­”æ¡ˆ"ç­‰å…³é”®è¯çš„è¡Œ
                if re.search(r'(total|answer|result|sum|final|is\s+\d+)', line, re.IGNORECASE):
                    # æå–è¯¥è¡Œä¸­çš„æ•°å­—
                    numbers = re.findall(r'-?\d+(?:,\d+)*(?:\.\d+)?', line)
                    if numbers:
                        # å–æœ€åä¸€ä¸ªæ•°å­—
                        answer = numbers[-1].replace(',', '')
                        try:
                            val = float(answer)
                            if 0 <= val < 1e10:
                                return answer
                        except ValueError:
                            continue
                
                # æŸ¥æ‰¾"X miles/dollars"æ ¼å¼
                match = re.search(r'(\d+(?:,\d+)*(?:\.\d+)?)\s+(?:miles?|dollars?|units?)', line, re.IGNORECASE)
                if match:
                    answer = match.group(1).replace(',', '')
                    try:
                        val = float(answer)
                        if 0 <= val < 1e10:
                            return answer
                    except ValueError:
                        continue
        
        # 3. æœ€åå°è¯•ï¼šæå–æ‰€æœ‰æ•°å­—ï¼Œè¿”å›æœ€åä¸€ä¸ªåˆç†çš„
        if not strict:
            all_numbers = re.findall(r'-?\d+(?:,\d+)*(?:\.\d+)?', text)
            if all_numbers:
                # ä»åå¾€å‰æ£€æŸ¥
                for num in reversed(all_numbers[-10:]):
                    cleaned = num.replace(',', '')
                    try:
                        val = float(cleaned)
                        # æ’é™¤è¿‡å°çš„æ•°å­—ï¼ˆå¯èƒ½æ˜¯æ­¥éª¤ç¼–å·ï¼‰å’Œè¿‡å¤§çš„å¼‚å¸¸å€¼
                        # åŒæ—¶æ’é™¤æ˜æ˜¾çš„ä¸­é—´è®¡ç®—å€¼ï¼ˆå¦‚180, 135ç­‰ï¼‰
                        if 1 <= val < 1e6:  # è°ƒæ•´èŒƒå›´
                            return cleaned
                    except ValueError:
                        continue
        
        return None
    
    @staticmethod
    def extract_from_probe_response(text: str) -> Optional[str]:
        """ä¸“é—¨ç”¨äºæå–æ¢é’ˆå“åº”ä¸­çš„ç­”æ¡ˆ"""
        if not text:
            return None
        
        # æ¸…ç†æ–‡æœ¬ï¼ˆä¸æ‰“å°è­¦å‘Šï¼‰
        text = TextCleaner.clean_response(text, verbose=False)
        
        # æ¢é’ˆå“åº”é€šå¸¸æ›´ç®€æ´ï¼Œä¼˜å…ˆåŒ¹é…å¼€å¤´çš„æ•°å­—
        patterns = [
            r'^\s*(-?\d+(?:,\d+)*(?:\.\d+)?)',  # å¼€å¤´çš„æ•°å­—
            r'[Aa]nswer:\s*(-?\d+(?:,\d+)*(?:\.\d+)?)',  # Answer: X
            r'(?:is|equals?|=)\s*(-?\d+(?:,\d+)*(?:\.\d+)?)',  # å¸¦ç­‰å·çš„
            r'(-?\d+(?:,\d+)*(?:\.\d+)?)\s+(?:miles?|dollars?)',  # X miles/dollars
            r'(-?\d+(?:,\d+)*(?:\.\d+)?)',  # ä»»æ„æ•°å­—
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                answer = match.group(1).replace(',', '').strip()
                try:
                    val = float(answer)
                    if 0 <= val < 1e10:
                        return answer
                except ValueError:
                    continue
        
        return None


# ============================================================================
# æ™ºèƒ½æ¢é’ˆç³»ç»Ÿ
# ============================================================================
class SmartProbeSystem:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.reasoning_markers = {
            'calculation': ['=', 'equals', 'total', 'sum', 'result', '+', '-', '*', '/', 'calculate'],
            'conclusion': ['therefore', 'thus', 'so', 'hence', 'finally', 'conclude', 'in conclusion'],
            'intermediate': ['step', 'first', 'next', 'then', 'now', 'assume', 'let'],
            'answer_signal': ['answer is', 'answer:', '####', '\\boxed', 'final answer']
        }
    
    def identify_reasoning_stage(self, text: str) -> str:
        text_lower = text.lower()
        last_100 = text_lower[-100:]
        
        for marker in self.reasoning_markers['answer_signal']:
            if marker in last_100: 
                return 'answer_signal'
        
        for marker in self.reasoning_markers['conclusion']:
            if marker in last_100: 
                return 'conclusion'
        
        for marker in self.reasoning_markers['calculation']:
            if marker in last_100: 
                return 'calculation'
        
        return 'intermediate'

    def probe_answer(self, full_sequence_ids: torch.Tensor, current_text: str, stage: str) -> CheckpointResult:
        """æ‰§è¡Œæ¢é’ˆæ£€æµ‹"""
        prompts = {
            'answer_signal': "\n\nThe final answer is: ",
            'conclusion': "\n\nTherefore, the answer is: ",
            'calculation': "\n\nThis equals: ",
            'intermediate': "\n\nThe current value is: "
        }
        probe_text = prompts.get(stage, "\n\nThe answer is: ")
        
        try:
            probe_tokens = self.tokenizer.encode(
                probe_text, 
                return_tensors='pt', 
                add_special_tokens=False
            ).to(self.model.device)
            
            probe_input = torch.cat([full_sequence_ids, probe_tokens], dim=-1)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    probe_input,
                    max_new_tokens=20,
                    do_sample=False,
                    output_scores=True,
                    return_dict_in_generate=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            if outputs.sequences.shape[1] > probe_input.shape[1]:
                gen_tokens = outputs.sequences[0][probe_input.shape[1]:]
                gen_text = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()
                
                # ä½¿ç”¨ä¸“é—¨çš„æ¢é’ˆç­”æ¡ˆæå–å™¨
                answer = AnswerExtractor.extract_from_probe_response(gen_text)
                if not answer:
                    answer = AnswerExtractor.extract_answer(gen_text)
                
                # è®¡ç®—ç†µ
                if outputs.scores and len(outputs.scores) > 0:
                    logits = outputs.scores[0][0]
                    probs = torch.softmax(logits, dim=-1)
                    log_probs = torch.log_softmax(logits, dim=-1)
                    entropy = -torch.sum(probs * log_probs).item()
                else:
                    entropy = 0.0

                return CheckpointResult(answer=answer, entropy=entropy)
                
        except Exception as e:
            print(f"âš ï¸ Probe error: {e}")
            
        return CheckpointResult(entropy=10.0)

# ============================================================================
# å¯è§†åŒ–å·¥å…· - å¢å¼ºç‰ˆ
# ============================================================================
class EntropyVisualizer:
    """ä¸“é—¨ç”¨äºç»˜åˆ¶ç†µå€¼å˜åŒ–å›¾"""
    
    STAGE_COLORS = {
        'intermediate': '#3498db',
        'calculation': '#f39c12',
        'conclusion': '#2ecc71',
        'answer_signal': '#e74c3c'
    }
    
    @staticmethod
    def smart_label_placement(steps, entropies, answers, ground_truth):
        """æ™ºèƒ½è®¡ç®—æ ‡æ³¨ä½ç½®ï¼Œé¿å…é‡å """
        positions = []
        
        for i, (step, ent, ans) in enumerate(zip(steps, entropies, answers)):
            if ans == 'None':
                positions.append(None)
                continue
            
            # è®¡ç®—åŸºç¡€åç§»
            base_offset = 15
            
            # æ£€æŸ¥ä¸å·²æœ‰æ ‡æ³¨çš„è·ç¦»
            conflicts = []
            for j, prev_pos in enumerate(positions):
                if prev_pos is None:
                    continue
                prev_step, prev_ent, prev_offset = prev_pos
                
                # è®¡ç®—æ°´å¹³å’Œå‚ç›´è·ç¦»
                h_dist = abs(step - prev_step)
                v_dist = abs(ent - prev_ent)
                
                if h_dist < 25 and v_dist < 0.5:  # å¤ªè¿‘äº†
                    conflicts.append(prev_offset)
            
            # æ ¹æ®å†²çªè°ƒæ•´åç§»
            if conflicts:
                # å°è¯•ä¸åŒçš„åç§»é‡
                possible_offsets = [15, -20, 25, -30, 35, -40]
                for offset in possible_offsets:
                    if offset not in conflicts:
                        y_offset = offset
                        break
                else:
                    # å¦‚æœéƒ½å†²çªï¼Œä½¿ç”¨äº¤æ›¿æ¨¡å¼
                    y_offset = 15 if i % 2 == 0 else -20
            else:
                # æ— å†²çªï¼Œä½¿ç”¨äº¤æ›¿æ¨¡å¼
                y_offset = 15 if i % 2 == 0 else -20
            
            positions.append((step, ent, y_offset))
        
        return positions
    
    @staticmethod
    def plot_single_entropy(ax, records: List[ProbeRecord], sample_id: int, 
                           ground_truth: str, is_correct: bool, final_answer: str):
        """åœ¨å•ä¸ªå­å›¾ä¸Šç»˜åˆ¶ç†µå€¼æ›²çº¿"""
        if not records:
            ax.text(0.5, 0.5, 'No probe data', ha='center', va='center')
            return

        steps = [r.step for r in records]
        entropies = [r.entropy for r in records]
        stages = [r.stage for r in records]
        answers = [str(r.answer) if r.answer else 'None' for r in records]

        # ç»˜åˆ¶ä¸»æ›²çº¿
        ax.plot(steps, entropies, color='gray', alpha=0.4, linestyle='--', 
                linewidth=1.5, zorder=1)

        # ç»˜åˆ¶æ•£ç‚¹
        for i, (step, ent, stage) in enumerate(zip(steps, entropies, stages)):
            color = EntropyVisualizer.STAGE_COLORS.get(stage, 'gray')
            size = 120 if stage == 'answer_signal' else 60
            ax.scatter(step, ent, color=color, s=size, zorder=2, 
                      edgecolors='white', linewidth=1.5, alpha=0.8)

        # æ™ºèƒ½æ ‡æ³¨æ‰€æœ‰æœ‰ç­”æ¡ˆçš„ç‚¹
        label_positions = EntropyVisualizer.smart_label_placement(
            steps, entropies, answers, ground_truth
        )
        
        for i, (step, ent, ans) in enumerate(zip(steps, entropies, answers)):
            if ans == 'None' or label_positions[i] is None:
                continue
            
            _, _, y_offset = label_positions[i]
            
            # åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
            ans_is_correct = (ans == str(ground_truth))
            ans_color = '#27ae60' if ans_is_correct else '#c0392b'
            weight = 'bold' if ans_is_correct else 'normal'
            
            # è°ƒæ•´å­—ä½“å¤§å°å’Œè¾¹æ¡†
            fontsize = 9 if ans_is_correct else 7
            linewidth = 2 if ans_is_correct else 1
            
            ax.annotate(
                f"{ans}", 
                (step, ent),
                xytext=(0, y_offset), 
                textcoords='offset points',
                ha='center', 
                fontsize=fontsize,
                color=ans_color,
                fontweight=weight,
                bbox=dict(
                    boxstyle="round,pad=0.3", 
                    fc="white", 
                    ec=ans_color, 
                    alpha=0.9, 
                    linewidth=linewidth
                ),
                zorder=10
            )

        # æ ‡é¢˜å’Œæ ‡ç­¾
        title_color = '#27ae60' if is_correct else '#c0392b'
        status = 'âœ“' if is_correct else 'âœ—'
        final_display = final_answer if final_answer else "None"
        
        ax.set_title(
            f"Sample #{sample_id} {status}\nGT: {ground_truth} | Final: {final_display}", 
            fontsize=10, fontweight='bold', color=title_color, pad=10
        )
        ax.set_xlabel("Token Steps", fontsize=9)
        ax.set_ylabel("Entropy", fontsize=9)
        
        # æ·»åŠ ä½ç†µé˜ˆå€¼çº¿
        ax.axhline(y=0.6, color='red', linestyle=':', alpha=0.3, linewidth=1, label='Low Entropy')
        
        # ç½‘æ ¼å’Œæ ·å¼
        ax.grid(True, alpha=0.2, linestyle='--')
        ax.set_facecolor('#fafafa')
        
    @staticmethod
    def plot_combined_entropy(correct_results: List[Dict], wrong_results: List[Dict], 
                             save_path: Path):
        """ç»˜åˆ¶æ­£ç¡®å’Œé”™è¯¯æ¡ˆä¾‹çš„å¯¹æ¯”å›¾"""
        if not HAS_PLOT:
            return

        n_correct = len(correct_results)
        n_wrong = len(wrong_results)
        total = n_correct + n_wrong
        
        if total == 0:
            print("âš ï¸ No results to plot")
            return

        cols = 3
        rows = (total + cols - 1) // cols
        
        fig = plt.figure(figsize=(18, 5 * rows))
        gs = GridSpec(rows, cols, figure=fig, hspace=0.4, wspace=0.25)
        
        # ç»˜åˆ¶æ­£ç¡®æ¡ˆä¾‹
        for idx, result in enumerate(correct_results):
            row = idx // cols
            col = idx % cols
            ax = fig.add_subplot(gs[row, col])
            
            records = [ProbeRecord.from_dict(r) for r in result['probe_history']]
            EntropyVisualizer.plot_single_entropy(
                ax, records,
                result['sample_id'],
                result['ground_truth'],
                True,
                result['final_answer']
            )
        
        # ç»˜åˆ¶é”™è¯¯æ¡ˆä¾‹
        offset = n_correct
        for idx, result in enumerate(wrong_results):
            plot_idx = offset + idx
            row = plot_idx // cols
            col = plot_idx % cols
            ax = fig.add_subplot(gs[row, col])
            
            records = [ProbeRecord.from_dict(r) for r in result['probe_history']]
            EntropyVisualizer.plot_single_entropy(
                ax, records,
                result['sample_id'],
                result['ground_truth'],
                False,
                result['final_answer']
            )
        
        # å…¨å±€å›¾ä¾‹
        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', 
                      markerfacecolor=color, markersize=10, label=stage)
            for stage, color in EntropyVisualizer.STAGE_COLORS.items()
        ]
        fig.legend(handles=legend_elements, loc='upper center', 
                  ncol=4, frameon=True, fontsize=11, 
                  bbox_to_anchor=(0.5, 0.99))
        
        fig.suptitle(
            f"Entropy Dynamics Analysis: {n_correct} Correct vs {n_wrong} Wrong",
            fontsize=16, fontweight='bold', y=0.997
        )
        
        plt.savefig(save_path, dpi=200, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š Combined plot saved to: {save_path}")

    @staticmethod
    def plot_statistics_comparison(correct_results: List[Dict], 
                                   wrong_results: List[Dict], 
                                   save_path: Path):
        """ç»˜åˆ¶ç»Ÿè®¡å¯¹æ¯”å›¾"""
        if not HAS_PLOT or (not correct_results and not wrong_results):
            return

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle("Correct vs Wrong Cases: Statistical Comparison", 
                    fontsize=14, fontweight='bold')

        def extract_stats(results):
            all_entropies = []
            probe_counts = []
            final_entropies = []
            
            for r in results:
                history = r['probe_history']
                if history:
                    entropies = [h['entropy'] for h in history]
                    all_entropies.extend(entropies)
                    probe_counts.append(len(history))
                    final_entropies.append(entropies[-1])
            
            return all_entropies, probe_counts, final_entropies

        correct_ent, correct_counts, correct_final = extract_stats(correct_results)
        wrong_ent, wrong_counts, wrong_final = extract_stats(wrong_results)

        # 1. ç†µå€¼åˆ†å¸ƒ
        ax1 = axes[0, 0]
        if correct_ent:
            ax1.hist(correct_ent, bins=20, alpha=0.6, color='green', label='Correct', density=True)
        if wrong_ent:
            ax1.hist(wrong_ent, bins=20, alpha=0.6, color='red', label='Wrong', density=True)
        ax1.set_xlabel("Entropy Value")
        ax1.set_ylabel("Density")
        ax1.set_title("Entropy Distribution")
        ax1.legend()
        ax1.grid(alpha=0.3)

        # 2. æ¢æµ‹æ¬¡æ•°
        ax2 = axes[0, 1]
        data_to_plot = []
        labels = []
        if correct_counts:
            data_to_plot.append(correct_counts)
            labels.append('Correct')
        if wrong_counts:
            data_to_plot.append(wrong_counts)
            labels.append('Wrong')
        if data_to_plot:
            bp = ax2.boxplot(data_to_plot, labels=labels, patch_artist=True)
            colors = ['lightgreen', 'lightcoral']
            for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):
                patch.set_facecolor(color)
        ax2.set_ylabel("Number of Probes")
        ax2.set_title("Probe Count Comparison")
        ax2.grid(alpha=0.3)

        # 3. æœ€ç»ˆç†µå€¼
        ax3 = axes[1, 0]
        x_pos = []
        heights = []
        colors = []
        if correct_final:
            x_pos.append(0)
            heights.append(np.mean(correct_final))
            colors.append('green')
        if wrong_final:
            x_pos.append(1)
            heights.append(np.mean(wrong_final))
            colors.append('red')
        if x_pos:
            ax3.bar(x_pos, heights, color=colors, alpha=0.7, width=0.6)
            ax3.set_xticks(x_pos)
            ax3.set_xticklabels(['Correct', 'Wrong'])
            ax3.set_ylabel("Average Final Entropy")
            ax3.set_title("Final Entropy Comparison")
            ax3.grid(alpha=0.3, axis='y')

        # 4. å‡†ç¡®ç‡
        ax4 = axes[1, 1]
        sizes = [len(correct_results), len(wrong_results)]
        labels_pie = ['Correct', 'Wrong']
        colors_pie = ['#27ae60', '#c0392b']
        if sum(sizes) > 0:
            ax4.pie(sizes, labels=labels_pie, colors=colors_pie, autopct='%1.1f%%',
                   startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
            ax4.set_title("Overall Accuracy")

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š Statistics plot saved to: {save_path}")

# ============================================================================
# å®éªŒè¿è¡Œå™¨
# ============================================================================
class EntropyExperimentRunner:
    def __init__(self):
        self.config = self._get_config()
        self.tokenizer = None
        self.model = None
        
    def _get_config(self):
        return {
            "model_name": "Qwen/Qwen2.5-7B-Instruct",
            "data_path": str(DATA_DIR / "gsm8k_test.json"),
            "sample_size": 20,
            "cooldown": 8,
            "max_tokens": 1024,
            "debug": True
        }

    def load_resources(self):
        print(f"ğŸ¤– Loading model: {self.config['model_name']}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['model_name'], 
            trust_remote_code=True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config['model_name'], 
            torch_dtype=torch.float16, 
            device_map="auto",
            trust_remote_code=True
        )
        self.model.eval()
        self.probe_system = SmartProbeSystem(self.model, self.tokenizer)

    def is_sentence_boundary(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¥å­è¾¹ç•Œ"""
        if not text: 
            return False
        return text.strip()[-1] in ['.', '!', '?', ':', '\n']
    
    def should_stop_generation(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢ç”Ÿæˆ"""
        # æ£€æŸ¥å¼‚å¸¸æ¨¡å¼
        for pattern in TextCleaner.STOP_PATTERNS:
            if pattern in text:
                return True
        return False

    def run_sample(self, sample_id: int, question: str, ground_truth: str):
        print(f"\n{'='*40}\nğŸ§ª Sample {sample_id}: {question[:50]}...")
        
        prompt = f"Question: {question}\nPlease solve this step by step.\nAnswer:"
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        input_ids = inputs.input_ids
        
        state = GenerationState(full_sequence_ids=input_ids)
        probe_records: List[ProbeRecord] = []
        
        last_probe_step = 0
        current_input_ids = input_ids
        past_key_values = None
        
        while state.tokens_used < self.config['max_tokens']:
            with torch.no_grad():
                outputs = self.model(
                    input_ids=current_input_ids,
                    past_key_values=past_key_values,
                    use_cache=True
                )
                next_token_logits = outputs.logits[:, -1, :]
                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)
                past_key_values = outputs.past_key_values
            
            state.full_sequence_ids = torch.cat([state.full_sequence_ids, next_token], dim=-1)
            current_input_ids = next_token
            state.tokens_used += 1
            
            new_text = self.tokenizer.decode(next_token[0], skip_special_tokens=True)
            state.full_response += new_text
            
            # æ£€æŸ¥åœæ­¢æ¡ä»¶
            if next_token.item() == self.tokenizer.eos_token_id:
                break
            if self.should_stop_generation(state.full_response):
                print(f"   âš ï¸ Early stop: Detected abnormal pattern")
                state.full_response = TextCleaner.clean_response(state.full_response, verbose=True)
                break

            # æ¢æµ‹é€»è¾‘
            is_boundary = self.is_sentence_boundary(state.full_response)
            is_cooldown_ok = (state.tokens_used - last_probe_step) >= self.config['cooldown']
            
            if is_boundary and is_cooldown_ok:
                stage = self.probe_system.identify_reasoning_stage(state.full_response)
                result = self.probe_system.probe_answer(
                    state.full_sequence_ids, 
                    state.full_response, 
                    stage
                )
                
                record = ProbeRecord(
                    step=state.tokens_used,
                    stage=stage,
                    answer=result.answer,
                    entropy=result.entropy,
                    text_segment=state.full_response[-30:].replace('\n', ' ')
                )
                probe_records.append(record)
                last_probe_step = state.tokens_used
                
                print(f"   ğŸ“ Step {state.tokens_used:3d} [{stage:12s}] "
                      f"Entropy: {result.entropy:.4f} | Ans: {result.answer}")

        # æ¸…ç†å“åº”ï¼ˆä¸æ‰“å°è­¦å‘Šï¼Œå› ä¸ºå·²ç»åœ¨ä¸Šé¢æ‰“å°è¿‡äº†ï¼‰
        state.full_response = TextCleaner.clean_response(state.full_response, verbose=False)
        
        # æå–æœ€ç»ˆç­”æ¡ˆ
        final_answer = AnswerExtractor.extract_answer(state.full_response)
        is_correct = (str(final_answer) == str(ground_truth))
        
        status = "âœ“ CORRECT" if is_correct else "âœ— WRONG"
        print(f"ğŸ {status} | Final: {final_answer} | GT: {ground_truth}")
        
        # è°ƒè¯•ä¿¡æ¯
        if self.config['debug'] and not final_answer:
            print(f"\nâš ï¸ Debug - Last 300 chars:\n{state.full_response[-300:]}\n")

        return {
            "sample_id": sample_id,
            "question": question,
            "ground_truth": ground_truth,
            "final_answer": final_answer,
            "correct": is_correct,
            "response": state.full_response,
            "probe_history": [record.to_dict() for record in probe_records]
        }

    def run(self):
        self.load_resources()
        
        with open(self.config['data_path'], 'r') as f:
            data = json.load(f)
        
        valid_data = []
        for item in data:
            ans = AnswerExtractor.extract_answer(item.get('answer', ''))
            if ans:
                item['clean_answer'] = ans
                valid_data.append(item)
        
        test_data = valid_data[:self.config['sample_size']]
        results = []
        
        for i, item in enumerate(test_data):
            res = self.run_sample(i, item['question'], item['clean_answer'])
            results.append(res)
        
        # åˆ†ç±»ç»“æœ
        correct_results = [r for r in results if r['correct']]
        wrong_results = [r for r in results if not r['correct']]
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Summary: {len(correct_results)} Correct | {len(wrong_results)} Wrong")
        print(f"   Accuracy: {len(correct_results)/len(results)*100:.1f}%")
        
        # ä¿å­˜JSON
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_path = RESULTS_DIR / f"entropy_analysis_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                "config": self.config,
                "summary": {
                    "total": len(results),
                    "correct": len(correct_results),
                    "wrong": len(wrong_results),
                    "accuracy": len(correct_results) / len(results) if results else 0
                },
                "results": results
            }, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Data saved to: {json_path}")
        
        # ç”Ÿæˆå¯è§†åŒ–
        if HAS_PLOT:
            combined_path = RESULTS_DIR / "plots" / f"entropy_combined_{timestamp}.png"
            EntropyVisualizer.plot_combined_entropy(
                correct_results, wrong_results, combined_path
            )
            
            stats_path = RESULTS_DIR / "plots" / f"entropy_statistics_{timestamp}.png"
            EntropyVisualizer.plot_statistics_comparison(
                correct_results, wrong_results, stats_path
            )

# ============================================================================
# Main
# ============================================================================
if __name__ == "__main__":
    runner = EntropyExperimentRunner()
    runner.run()
