"""
æ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬
æ”¯æŒ GSM8K å’Œ MATH Dataset
ä¸éœ€è¦GPUèµ„æº,å¯ä»¥åœ¨CPUç¯å¢ƒä¸‹è¿è¡Œ


# ä¸‹è½½ä¸¤ä¸ªæ•°æ®é›†
python data_preparation.py --dataset both

# åªä¸‹è½½ GSM8K
python data_preparation.py --dataset gsm8k

# åªä¸‹è½½ MATH
python data_preparation.py --dataset math

# è·³è¿‡ä¸‹è½½,ä»…å¤„ç†ç°æœ‰æ•°æ®
python data_preparation.py --skip-download


"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
import pickle
from datetime import datetime

# è®¾ç½® Hugging Face é•œåƒ(é‡è¦!)
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from datasets import load_dataset

# è®¾ç½®è·¯å¾„
BASE_DIR = Path("/root/autodl-tmp")
DATA_DIR = BASE_DIR / "data"
CONFIG_DIR = BASE_DIR / "config"

# åˆ›å»ºç›®å½•
DATA_DIR.mkdir(parents=True, exist_ok=True)
CONFIG_DIR.mkdir(parents=True, exist_ok=True)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(BASE_DIR / 'data_preparation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def download_gsm8k_dataset():
    """ä¸‹è½½GSM8Kæ•°æ®é›†"""
    logger.info("å¼€å§‹ä¸‹è½½GSM8Kæ•°æ®é›†...")
    
    try:
        # ä¸‹è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_dataset = load_dataset("gsm8k", "main", split="train")
        test_dataset = load_dataset("gsm8k", "main", split="test")
        
        logger.info(f"GSM8Kè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"GSM8Kæµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼,ä¾¿äºåç»­å¤„ç†
        train_data = []
        for item in train_dataset:
            train_data.append({
                "question": item["question"],
                "answer": item["answer"],
                "dataset": "gsm8k"
            })
        
        test_data = []
        for item in test_dataset:
            test_data.append({
                "question": item["question"],
                "answer": item["answer"],
                "dataset": "gsm8k"
            })
        
        # ä¿å­˜è®­ç»ƒé›†
        train_file = DATA_DIR / "gsm8k_train.json"
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        logger.info(f"GSM8Kè®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_file}")
        
        # ä¿å­˜æµ‹è¯•é›†
        test_file = DATA_DIR / "gsm8k_test.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        logger.info(f"GSM8Kæµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
        
        return train_file, test_file
        
    except Exception as e:
        logger.error(f"GSM8Kæ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
        raise

def download_math_dataset():
    """ä¸‹è½½MATHæ•°æ®é›†"""
    logger.info("å¼€å§‹ä¸‹è½½MATHæ•°æ®é›†...")
    
    try:
        # MATHæ•°æ®é›†åœ¨ Hugging Face ä¸Šçš„åç§°æ˜¯ "hendrycks/competition_math"
        # æˆ–è€… "lighteval/MATH"
        train_dataset = load_dataset("lighteval/MATH", split="train")
        test_dataset = load_dataset("lighteval/MATH", split="test")
        
        logger.info(f"MATHè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        logger.info(f"MATHæµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
        # ç»Ÿè®¡å„ä¸ªéš¾åº¦å’Œç±»åˆ«
        train_levels = {}
        train_types = {}
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        train_data = []
        for item in train_dataset:
            # MATHæ•°æ®é›†åŒ…å«: problem, solution, level, type
            entry = {
                "question": item["problem"],
                "answer": item["solution"],
                "level": item.get("level", "unknown"),
                "type": item.get("type", "unknown"),
                "dataset": "math"
            }
            train_data.append(entry)
            
            # ç»Ÿè®¡
            level = entry["level"]
            prob_type = entry["type"]
            train_levels[level] = train_levels.get(level, 0) + 1
            train_types[prob_type] = train_types.get(prob_type, 0) + 1
        
        test_data = []
        test_levels = {}
        test_types = {}
        
        for item in test_dataset:
            entry = {
                "question": item["problem"],
                "answer": item["solution"],
                "level": item.get("level", "unknown"),
                "type": item.get("type", "unknown"),
                "dataset": "math"
            }
            test_data.append(entry)
            
            # ç»Ÿè®¡
            level = entry["level"]
            prob_type = entry["type"]
            test_levels[level] = test_levels.get(level, 0) + 1
            test_types[prob_type] = test_types.get(prob_type, 0) + 1
        
        # ä¿å­˜è®­ç»ƒé›†
        train_file = DATA_DIR / "math_train.json"
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        logger.info(f"MATHè®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_file}")
        
        # ä¿å­˜æµ‹è¯•é›†
        test_file = DATA_DIR / "math_test.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        logger.info(f"MATHæµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n" + "="*60)
        logger.info("MATHæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"\nè®­ç»ƒé›†éš¾åº¦åˆ†å¸ƒ:")
        for level, count in sorted(train_levels.items()):
            logger.info(f"  {level}: {count}")
        
        logger.info(f"\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
        for prob_type, count in sorted(train_types.items()):
            logger.info(f"  {prob_type}: {count}")
        
        logger.info(f"\næµ‹è¯•é›†éš¾åº¦åˆ†å¸ƒ:")
        for level, count in sorted(test_levels.items()):
            logger.info(f"  {level}: {count}")
        
        logger.info(f"\næµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ:")
        for prob_type, count in sorted(test_types.items()):
            logger.info(f"  {prob_type}: {count}")
        logger.info("="*60 + "\n")
        
        return train_file, test_file
        
    except Exception as e:
        logger.error(f"MATHæ•°æ®é›†ä¸‹è½½å¤±è´¥: {e}")
        logger.info("æç¤º: å¦‚æœä¸‹è½½å¤±è´¥,è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨VPN")
        raise

def create_sample_datasets(dataset_name="gsm8k"):
    """åˆ›å»ºä¸åŒå¤§å°çš„é‡‡æ ·æ•°æ®é›†ç”¨äºæµ‹è¯•
    
    Args:
        dataset_name: "gsm8k" æˆ– "math"
    """
    logger.info(f"åˆ›å»º{dataset_name.upper()}é‡‡æ ·æ•°æ®é›†...")
    
    test_file = DATA_DIR / f"{dataset_name}_test.json"
    if not test_file.exists():
        logger.error(f"{dataset_name.upper()}æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨,è¯·å…ˆè¿è¡Œä¸‹è½½")
        return
    
    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # åˆ›å»ºä¸åŒå¤§å°çš„é‡‡æ ·(åŒ…å«æ›´å°çš„æ ·æœ¬ç”¨äºå¿«é€Ÿæµ‹è¯•)
    sample_sizes = [5, 10, 20, 50, 100, 200]
    
    for size in sample_sizes:
        if size <= len(test_data):
            sample_data = test_data[:size]
            sample_file = DATA_DIR / f"{dataset_name}_test_sample_{size}.json"
            
            with open(sample_file, 'w', encoding='utf-8') as f:
                json.dump(sample_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"{dataset_name.upper()}é‡‡æ ·æ•°æ®é›† ({size}æ¡) å·²ä¿å­˜åˆ°: {sample_file}")

def create_math_subset_by_difficulty():
    """æ ¹æ®éš¾åº¦åˆ›å»ºMATHæ•°æ®é›†çš„å­é›†"""
    logger.info("åˆ›å»ºMATHæ•°æ®é›†éš¾åº¦å­é›†...")
    
    test_file = DATA_DIR / "math_test.json"
    if not test_file.exists():
        logger.warning("MATHæµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨,è·³è¿‡éš¾åº¦å­é›†åˆ›å»º")
        return
    
    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # æŒ‰éš¾åº¦åˆ†ç»„
    levels = {}
    for item in test_data:
        level = item.get("level", "unknown")
        if level not in levels:
            levels[level] = []
        levels[level].append(item)
    
    # ä¸ºæ¯ä¸ªéš¾åº¦åˆ›å»ºå­é›†
    for level, items in levels.items():
        level_file = DATA_DIR / f"math_test_level_{level}.json"
        with open(level_file, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)
        logger.info(f"MATHéš¾åº¦å­é›† Level {level} ({len(items)}æ¡) å·²ä¿å­˜åˆ°: {level_file}")

def create_math_subset_by_type():
    """æ ¹æ®ç±»åˆ«åˆ›å»ºMATHæ•°æ®é›†çš„å­é›†"""
    logger.info("åˆ›å»ºMATHæ•°æ®é›†ç±»åˆ«å­é›†...")
    
    test_file = DATA_DIR / "math_test.json"
    if not test_file.exists():
        logger.warning("MATHæµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨,è·³è¿‡ç±»åˆ«å­é›†åˆ›å»º")
        return
    
    with open(test_file, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    types = {}
    for item in test_data:
        prob_type = item.get("type", "unknown")
        if prob_type not in types:
            types[prob_type] = []
        types[prob_type].append(item)
    
    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ›å»ºå­é›†
    for prob_type, items in types.items():
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_type = prob_type.replace(" ", "_").replace("/", "_")
        type_file = DATA_DIR / f"math_test_type_{safe_type}.json"
        with open(type_file, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)
        logger.info(f"MATHç±»åˆ«å­é›† {prob_type} ({len(items)}æ¡) å·²ä¿å­˜åˆ°: {type_file}")

def preprocess_answers(dataset_name="gsm8k"):
    """é¢„å¤„ç†ç­”æ¡ˆ,æå–æ•°å€¼ç­”æ¡ˆ
    
    Args:
        dataset_name: "gsm8k" æˆ– "math"
    """
    logger.info(f"é¢„å¤„ç†{dataset_name.upper()}ç­”æ¡ˆ...")
    
    import re
    
    def extract_numerical_answer_gsm8k(answer_text: str) -> str:
        """ä»GSM8Kç­”æ¡ˆæ–‡æœ¬ä¸­æå–æ•°å€¼"""
        # æŸ¥æ‰¾ #### åçš„æ•°å­—
        match = re.search(r'####\s*([+-]?\d+(?:\.\d+)?)', answer_text)
        if match:
            num = float(match.group(1))
            return str(int(num)) if num.is_integer() else str(num)
        return None
    
    def extract_numerical_answer_math(answer_text: str) -> str:
        """ä»MATHç­”æ¡ˆæ–‡æœ¬ä¸­æå–æ•°å€¼
        MATHæ•°æ®é›†çš„ç­”æ¡ˆé€šå¸¸åœ¨ \\boxed{} ä¸­
        """
        # æŸ¥æ‰¾ \boxed{} ä¸­çš„å†…å®¹
        match = re.search(r'\\boxed\{([^}]+)\}', answer_text)
        if match:
            answer = match.group(1).strip()
            # å°è¯•æå–çº¯æ•°å­—
            num_match = re.search(r'([+-]?\d+(?:\.\d+)?)', answer)
            if num_match:
                num = float(num_match.group(1))
                return str(int(num)) if num.is_integer() else str(num)
            # å¦‚æœä¸æ˜¯çº¯æ•°å­—,è¿”å›åŸå§‹ç­”æ¡ˆ
            return answer
        return None
    
    # é€‰æ‹©æå–å‡½æ•°
    extract_func = extract_numerical_answer_gsm8k if dataset_name == "gsm8k" else extract_numerical_answer_math
    
    # å¤„ç†è®­ç»ƒé›†
    train_file = DATA_DIR / f"{dataset_name}_train.json"
    if train_file.exists():
        with open(train_file, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        for item in train_data:
            item['numerical_answer'] = extract_func(item['answer'])
        
        processed_train_file = DATA_DIR / f"{dataset_name}_train_processed.json"
        with open(processed_train_file, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å¤„ç†åçš„{dataset_name.upper()}è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {processed_train_file}")
    
    # å¤„ç†æµ‹è¯•é›†
    test_file = DATA_DIR / f"{dataset_name}_test.json"
    if test_file.exists():
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        for item in test_data:
            item['numerical_answer'] = extract_func(item['answer'])
        
        processed_test_file = DATA_DIR / f"{dataset_name}_test_processed.json"
        with open(processed_test_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å¤„ç†åçš„{dataset_name.upper()}æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {processed_test_file}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        valid_answers = sum(1 for item in test_data if item['numerical_answer'] is not None)
        logger.info(f"{dataset_name.upper()}æµ‹è¯•é›†ä¸­æœ‰æ•ˆæ•°å€¼ç­”æ¡ˆ: {valid_answers}/{len(test_data)}")

def create_config_file():
    """åˆ›å»ºé…ç½®æ–‡ä»¶(ä¸å®éªŒè„šæœ¬å…¼å®¹çš„æ ¼å¼)"""
    logger.info("åˆ›å»ºé…ç½®æ–‡ä»¶...")
    
    config = {
        "model_configs": {
            "qwen2.5-1.5b": {
                "name": "Qwen/Qwen2.5-1.5B-Instruct",
                "description": "è½»é‡çº§ - é€‚åˆå¿«é€Ÿæµ‹è¯•"
            },
            "qwen2.5-3b": {
                "name": "Qwen/Qwen2.5-3B-Instruct", 
                "description": "å¹³è¡¡æ€§èƒ½ - æ¨èæ—¥å¸¸ä½¿ç”¨"
            },
            "qwen2.5-7b": {
                "name": "Qwen/Qwen2.5-7B-Instruct",
                "description": "é«˜æ€§èƒ½ - è¾ƒæ…¢ä½†æ›´å‡†ç¡®"
            },
            "qwen2.5-14b": {
                "name": "Qwen/Qwen2.5-14B-Instruct",
                "description": "æœ€é«˜æ€§èƒ½ - éœ€è¦å¤§æ˜¾å­˜"
            }
        },
        "active_model": "qwen2.5-1.5b",  # é»˜è®¤ä½¿ç”¨1.5Bæ¨¡å‹
        "active_dataset": "gsm8k",  # é»˜è®¤ä½¿ç”¨GSM8Kæ•°æ®é›†,å¯é€‰: "gsm8k", "math"
        "experiment": {
            "sample_size": 50,           # é»˜è®¤50ä¸ªæ ·æœ¬,é€‚åˆå®éªŒ
            "verbose": False,            # é»˜è®¤ä¸æ˜¾ç¤ºè¯¦ç»†è¾“å‡º,é¿å…åˆ·å±
            "save_results": True,        # é»˜è®¤ä¿å­˜ç»“æœ
            "max_reasoning_steps": 10,   # æ•°å­¦é¢˜ä¸€èˆ¬ä¸éœ€è¦å¤ªå¤šæ­¥éª¤
            "max_new_tokens": 200,       # æ§åˆ¶ç”Ÿæˆé•¿åº¦
            "temperature": 0.0,          # ç¡®ä¿è¾“å‡ºç¨³å®š
            "do_sample": False          # è´ªå©ªè§£ç ,ç¡®ä¿ä¸€è‡´æ€§
        },
        "paths": {
            "data_dir": str(DATA_DIR),
            "results_dir": str(BASE_DIR / "results"),
            # GSM8Kè·¯å¾„
            "gsm8k_test_data": str(DATA_DIR / "gsm8k_test_processed.json"),
            "gsm8k_raw_test_data": str(DATA_DIR / "gsm8k_test.json"),
            # MATHè·¯å¾„
            "math_test_data": str(DATA_DIR / "math_test_processed.json"),
            "math_raw_test_data": str(DATA_DIR / "math_test.json"),
            # é‡‡æ ·æ•°æ®è·¯å¾„
            "sample_data_5": str(DATA_DIR / "gsm8k_test_sample_5.json"),
            "sample_data_10": str(DATA_DIR / "gsm8k_test_sample_10.json"),
            "sample_data_20": str(DATA_DIR / "gsm8k_test_sample_20.json"),
            "sample_data_50": str(DATA_DIR / "gsm8k_test_sample_50.json"),
            "sample_data_100": str(DATA_DIR / "gsm8k_test_sample_100.json"),
            "sample_data_200": str(DATA_DIR / "gsm8k_test_sample_200.json")
        },
        "generation": {
            "max_length": 512,           # è¾“å…¥æœ€å¤§é•¿åº¦
            "max_new_tokens": 200,       # ç”Ÿæˆæœ€å¤§é•¿åº¦
            "do_sample": False,          # ä¸é‡‡æ ·,ä½¿ç”¨è´ªå©ªè§£ç 
            "temperature": 0.0,          # æ¸©åº¦ä¸º0,ç¡®ä¿ä¸€è‡´æ€§
            "top_p": 1.0,               # ä¸ä½¿ç”¨top-p
            "num_return_sequences": 1    # åªè¿”å›ä¸€ä¸ªåºåˆ—
        },
        "halt_cot": {
            "entropy_threshold_strict": 0.3,
            "entropy_threshold_loose": 0.8,
            "k_strict": 2,
            "k_normal": 4,
            "k_conservative": 8,
            "min_reasoning_steps": 3,
            "max_reasoning_steps": 15,
            "entropy_history_size": 10,
            "confidence_decay": 0.95
        },
        "model": {
            "device": "cuda",
            "torch_dtype": "bfloat16"
        },
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "version": "1.2",
            "purpose": "GSM8Kå’ŒMATHæ•°æ®é›†HALT-CoTå®éªŒ",
            "supported_datasets": ["gsm8k", "math"]
        }
    }
    
    config_file = CONFIG_DIR / "config.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    logger.info(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ°: {config_file}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("\n" + "="*60)
    print("ğŸ¤– å¯ç”¨æ¨¡å‹é…ç½®:")
    for key, model_config in config['model_configs'].items():
        marker = "ğŸ‘ˆ [å½“å‰]" if key == config["active_model"] else ""
        print(f"  {key}: {model_config['name']} {marker}")
        print(f"    {model_config['description']}")
    
    print("\nğŸ“Š æ”¯æŒçš„æ•°æ®é›†:")
    for dataset in config['metadata']['supported_datasets']:
        marker = "ğŸ‘ˆ [å½“å‰]" if dataset == config["active_dataset"] else ""
        print(f"  {dataset.upper()} {marker}")
    
    print("\nâš™ï¸ å®éªŒé…ç½®:")
    print(f"  æ ·æœ¬æ•°é‡: {config['experiment']['sample_size']}")
    print(f"  è¯¦ç»†è¾“å‡º: {config['experiment']['verbose']}")
    print(f"  æœ€å¤§æ¨ç†æ­¥æ•°: {config['experiment']['max_reasoning_steps']}")
    print("="*60)
    
    return config_file

def download_model_cache():
    """é¢„ä¸‹è½½æ¨¡å‹ç¼“å­˜(å¯é€‰)"""
    logger.info("é¢„ä¸‹è½½æ¨¡å‹ç¼“å­˜...")
    
    try:
        from transformers import AutoTokenizer, AutoConfig
        
        # ä¸‹è½½1.5Bæ¨¡å‹(é»˜è®¤æ¨¡å‹)
        model_name = "Qwen/Qwen2.5-1.5B-Instruct"
        cache_dir = BASE_DIR / "model_cache"
        cache_dir.mkdir(exist_ok=True)
        
        # ä¸‹è½½tokenizer
        logger.info("ä¸‹è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        # ä¸‹è½½æ¨¡å‹é…ç½®
        logger.info("ä¸‹è½½æ¨¡å‹é…ç½®...")
        config = AutoConfig.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        logger.info(f"æ¨¡å‹ç¼“å­˜å·²ä¸‹è½½åˆ°: {cache_dir}")
        
    except Exception as e:
        logger.warning(f"æ¨¡å‹ç¼“å­˜ä¸‹è½½å¤±è´¥(å¯å¿½ç•¥): {e}")

def verify_data_integrity():
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    logger.info("éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    files_to_check = [
        # GSM8Kæ–‡ä»¶
        DATA_DIR / "gsm8k_train.json",
        DATA_DIR / "gsm8k_test.json",
        DATA_DIR / "gsm8k_train_processed.json", 
        DATA_DIR / "gsm8k_test_processed.json",
        DATA_DIR / "gsm8k_test_sample_5.json",
        DATA_DIR / "gsm8k_test_sample_10.json",
        DATA_DIR / "gsm8k_test_sample_20.json",
        DATA_DIR / "gsm8k_test_sample_50.json",
        DATA_DIR / "gsm8k_test_sample_100.json",
        DATA_DIR / "gsm8k_test_sample_200.json",
        # MATHæ–‡ä»¶
        DATA_DIR / "math_train.json",
        DATA_DIR / "math_test.json",
        DATA_DIR / "math_train_processed.json",
        DATA_DIR / "math_test_processed.json",
        # é…ç½®æ–‡ä»¶
        CONFIG_DIR / "config.json"
    ]
    
    all_good = True
    for file_path in files_to_check:
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                size_info = len(data) if isinstance(data, list) else 'OK'
                logger.info(f"âœ… {file_path.name}: {size_info}")
            except Exception as e:
                logger.error(f"âŒ {file_path.name}: æ–‡ä»¶æŸå - {e}")
                all_good = False
        else:
            # MATHæ•°æ®é›†æ–‡ä»¶æ˜¯å¯é€‰çš„
            if "math" in file_path.name:
                logger.warning(f"âš ï¸  {file_path.name}: æ–‡ä»¶ä¸å­˜åœ¨ (MATHæ•°æ®é›†å¯é€‰)")
            else:
                logger.error(f"âŒ {file_path.name}: æ–‡ä»¶ä¸å­˜åœ¨")
                all_good = False
    
    if all_good:
        logger.info("âœ… æ‰€æœ‰å¿…éœ€çš„æ•°æ®æ–‡ä»¶éªŒè¯é€šè¿‡!")
    else:
        logger.error("âŒ éƒ¨åˆ†æ•°æ®æ–‡ä»¶æœ‰é—®é¢˜,è¯·é‡æ–°è¿è¡Œæ•°æ®å‡†å¤‡")
    
    return all_good

def update_existing_config():
    """æ›´æ–°ç°æœ‰é…ç½®æ–‡ä»¶ä»¥ç¡®ä¿å…¼å®¹æ€§"""
    config_file = CONFIG_DIR / "config.json"
    
    if config_file.exists():
        logger.info("å‘ç°ç°æœ‰é…ç½®æ–‡ä»¶,æ›´æ–°ä»¥ç¡®ä¿å…¼å®¹æ€§...")
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # æ·»åŠ æ•°æ®é›†é€‰æ‹©
            if 'active_dataset' not in config:
                config['active_dataset'] = 'gsm8k'
                logger.info("æ·»åŠ active_dataseté…ç½®: gsm8k")
            
            # ç¡®ä¿experimenté…ç½®åŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
            if 'experiment' not in config:
                config['experiment'] = {}
            
            experiment_defaults = {
                "sample_size": 50,
                "verbose": False,
                "save_results": True,
                "max_reasoning_steps": 10,
                "max_new_tokens": 200,
                "temperature": 0.0,
                "do_sample": False
            }
            
            for key, value in experiment_defaults.items():
                if key not in config['experiment']:
                    config['experiment'][key] = value
                    logger.info(f"æ·»åŠ ç¼ºå¤±çš„é…ç½®é¡¹: experiment.{key} = {value}")
            
            # ç¡®ä¿pathsé…ç½®å®Œæ•´
            if 'paths' not in config:
                config['paths'] = {}
            
            path_defaults = {
                "data_dir": str(DATA_DIR),
                "results_dir": str(BASE_DIR / "results"),
                "gsm8k_test_data": str(DATA_DIR / "gsm8k_test_processed.json"),
                "gsm8k_raw_test_data": str(DATA_DIR / "gsm8k_test.json"),
                "math_test_data": str(DATA_DIR / "math_test_processed.json"),
                "math_raw_test_data": str(DATA_DIR / "math_test.json")
            }
            
            for key, value in path_defaults.items():
                if key not in config['paths']:
                    config['paths'][key] = value
                    logger.info(f"æ·»åŠ ç¼ºå¤±çš„è·¯å¾„é…ç½®: paths.{key}")
            
            # ç¡®ä¿generationé…ç½®å­˜åœ¨
            if 'generation' not in config:
                config['generation'] = {
                    "max_length": 512,
                    "max_new_tokens": 200,
                    "do_sample": False,
                    "temperature": 0.0,
                    "top_p": 1.0,
                    "num_return_sequences": 1
                }
                logger.info("æ·»åŠ generationé…ç½®")
            
            # æ›´æ–°metadata
            if 'metadata' not in config:
                config['metadata'] = {}
            config['metadata']['updated_at'] = datetime.now().isoformat()
            config['metadata']['version'] = "1.2"
            config['metadata']['supported_datasets'] = ["gsm8k", "math"]
            
            # ä¿å­˜æ›´æ–°åçš„é…ç½®
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            logger.info("é…ç½®æ–‡ä»¶å·²æ›´æ–°")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹")
    logger.info("=" * 60)
    
    import argparse
    parser = argparse.ArgumentParser(description='æ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†è„šæœ¬')
    parser.add_argument('--dataset', type=str, default='both', 
                       choices=['gsm8k', 'math', 'both'],
                       help='é€‰æ‹©è¦ä¸‹è½½çš„æ•°æ®é›†: gsm8k, math, æˆ– both')
    parser.add_argument('--skip-download', action='store_true',
                       help='è·³è¿‡ä¸‹è½½,ä»…å¤„ç†ç°æœ‰æ•°æ®')
    args = parser.parse_args()
    
    try:
        # 0. é¦–å…ˆå°è¯•æ›´æ–°ç°æœ‰é…ç½®
        config_updated = update_existing_config()
        
        if not args.skip_download:
            # 1. ä¸‹è½½æ•°æ®é›†
            if args.dataset in ['gsm8k', 'both']:
                download_gsm8k_dataset()
                create_sample_datasets('gsm8k')
                preprocess_answers('gsm8k')
            
            if args.dataset in ['math', 'both']:
                try:
                    download_math_dataset()
                    create_sample_datasets('math')
                    create_math_subset_by_difficulty()
                    create_math_subset_by_type()
                    preprocess_answers('math')
                except Exception as e:
                    logger.error(f"MATHæ•°æ®é›†å¤„ç†å¤±è´¥: {e}")
                    logger.info("ç»§ç»­å¤„ç†å…¶ä»–æ•°æ®é›†...")
        
        # 2. åˆ›å»º/æ›´æ–°é…ç½®æ–‡ä»¶
        if not config_updated:
            create_config_file()
        
        # 3. é¢„ä¸‹è½½æ¨¡å‹ç¼“å­˜(å¯é€‰)
        download_model_cache()
        
        # 4. éªŒè¯æ•°æ®å®Œæ•´æ€§
        verify_data_integrity()
        
        logger.info("=" * 60)
        logger.info("æ•°æ®å‡†å¤‡å®Œæˆ! å¯ä»¥è¿è¡Œæ¨¡å‹ä»£ç äº†")
        logger.info("=" * 60)
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = BASE_DIR / "results"
        results_dir.mkdir(exist_ok=True)
        
        logger.info(f"æ•°æ®å­˜å‚¨ä½ç½®: {DATA_DIR}")
        logger.info(f"é…ç½®æ–‡ä»¶ä½ç½®: {CONFIG_DIR}")
        logger.info(f"ç»“æœä¿å­˜ä½ç½®: {results_dir}")
        
        print("\nğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆ!")
        print("ğŸ’¡ æç¤º:")
        print("   1. å¯ä»¥ä½¿ç”¨ config_manager.py æ¥è°ƒæ•´å®éªŒå‚æ•°")
        print("   2. è¿è¡Œ experiment_runner.py å¼€å§‹å®éªŒ")
        print("   3. å»ºè®®å…ˆç”¨å°æ ·æœ¬æµ‹è¯• (sample_size=10)")
        print(f"   4. å½“å‰æ”¯æŒçš„æ•°æ®é›†: GSM8K" + (" å’Œ MATH" if args.dataset in ['math', 'both'] else ""))
        
    except Exception as e:
        logger.error(f"æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()
