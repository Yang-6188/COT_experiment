"""
ç†µå˜åŒ–ç‡åˆ†æå¯è§†åŒ–è„šæœ¬
ä½œè€…: Assistant
æ—¥æœŸ: 2025-12-04
ç”¨é€”: åˆ†æHALT-CoTå®éªŒä¸­çš„ç†µå˜åŒ–è¶‹åŠ¿å’Œæ¢é’ˆå‡†ç¡®æ€§
"""

import json
# ===== å…³é”®ä¿®æ”¹å¼€å§‹ =====
import matplotlib
matplotlib.use('Agg')  # å¿…é¡»åœ¨ import pyplot ä¹‹å‰
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')  # æå‰è¿‡æ»¤è­¦å‘Š
# ===== å…³é”®ä¿®æ”¹ç»“æŸ =====

import numpy as np
from matplotlib.patches import Rectangle
import matplotlib.patches as mpatches
from pathlib import Path
import sys

# ============================================================
# é…ç½®åŒºåŸŸ - åœ¨è¿™é‡Œè®¾ç½®ä½ çš„æ–‡ä»¶è·¯å¾„
# ============================================================
JSON_FILE_PATH = "/root/autodl-tmp/results/halt_cot_qwen2.5-7b_smart_50samples_20251204_171920.json"
OUTPUT_DIR = "./output"
SAMPLES_TO_PLOT = "all"  # æ”¹ä¸º "all" è¡¨ç¤ºæ‰€æœ‰æ ·æœ¬
SAMPLES_PER_FIGURE = 5  # æ¯å¼ å›¾æ˜¾ç¤º5ä¸ªæ ·æœ¬
PLOT_ALL_STATISTICS = True
DPI = 300
FIGURE_FORMAT = 'png'
# ============================================================


# åœ¨æ–‡ä»¶å¼€å¤´ï¼Œsetup_chinese_font() å‡½æ•°ä¹‹å‰æ·»åŠ 
def clean_latex_text(text):
    """æ¸…ç†æ–‡æœ¬ä¸­çš„LaTeXç¬¦å·ï¼Œé¿å…matplotlibè§£æé”™è¯¯"""
    import re
    if text is None:
        return ""
    text = str(text)
    
    # ç§»é™¤ç¾å…ƒç¬¦å·ï¼ˆLaTeXæ•°å­¦æ¨¡å¼æ ‡è®°ï¼‰
    text = text.replace('$', '')
    
    # ç§»é™¤æˆ–æ›¿æ¢LaTeXå‘½ä»¤
    # ä¾‹å¦‚: \sqrt, \log, \frac ç­‰
    text = re.sub(r'\\sqrt\{([^}]*)\}', r'sqrt(\1)', text)
    text = re.sub(r'\\log_\{([^}]*)\}', r'log_\1', text)
    text = re.sub(r'\\frac\{([^}]*)\}\{([^}]*)\}', r'(\1)/(\2)', text)
    text = re.sub(r'\\[a-zA-Z]+', '', text)  # ç§»é™¤å…¶ä»–LaTeXå‘½ä»¤
    
    # æ›¿æ¢èŠ±æ‹¬å·
    text = text.replace('{', '(').replace('}', ')')
    
    # ç§»é™¤ä¸‹åˆ’çº¿å’Œä¸Šæ ‡ç¬¦å·ï¼ˆå¦‚æœä¸åœ¨æ•°å­¦æ¨¡å¼ä¸­ï¼‰
    # text = text.replace('_', ' ').replace('^', ' ')
    
    return text




def setup_chinese_font():
    """é…ç½®ä¸­æ–‡å­—ä½“ - ç®€åŒ–ç‰ˆ"""
    # ç›´æ¥è®¾ç½®ï¼Œä¸è¦åœ¨å‡½æ•°å†…éƒ¨è¿‡æ»¤è­¦å‘Šï¼ˆå·²ç»åœ¨å¤–é¢è¿‡æ»¤äº†ï¼‰
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'WenQuanYi Zen Hei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ“ ä¸­æ–‡å­—ä½“é…ç½®å®Œæˆ")

def load_data(file_path):
    """åŠ è½½JSONæ•°æ®"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ“ æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
        return data
    except FileNotFoundError:
        print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"âœ— é”™è¯¯: æ–‡ä»¶ {file_path} ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
        sys.exit(1)


def create_output_dir(output_dir):
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    print(f"âœ“ è¾“å‡ºç›®å½•: {output_dir}")


def print_experiment_info(data):
    """æ‰“å°å®éªŒåŸºæœ¬ä¿¡æ¯"""
    print("\n" + "=" * 70)
    print("ğŸ“Š HALT-CoT å®éªŒç»Ÿè®¡ä¿¡æ¯")
    print("=" * 70)
    
    exp_info = data['experiment_info']
    stats = data['statistics']
    
    print(f"ğŸ¤– æ¨¡å‹: {exp_info['model']}")
    print(f"ğŸ“… æ—¶é—´æˆ³: {exp_info['timestamp']}")
    print(f"ğŸ“ æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"âœ… æ­£ç¡®æ•°: {stats['correct_samples']}")
    print(f"ğŸ¯ å‡†ç¡®ç‡: {stats['accuracy']*100:.2f}%")
    print(f"ğŸ›‘ æ—©åœç‡: {stats['early_stop_rate']*100:.2f}%")
    print(f"â±ï¸  å¹³å‡ç”¨æ—¶: {stats['avg_time_per_sample']:.2f}ç§’/æ ·æœ¬")
    print(f"ğŸ’¬ å¹³å‡Token: {stats['avg_tokens_per_sample']:.2f}ä¸ª/æ ·æœ¬")
    print(f"ğŸ“Š TokenèŒƒå›´: {stats['min_tokens']} - {stats['max_tokens']}")
    print(f"ğŸ“‰ å¹³å‡ç†µ: {stats['avg_entropy']:.4f}")
    
    if stats.get('halt_reasons'):
        print(f"\nğŸ” æ—©åœåŸå› åˆ†å¸ƒ:")
        for reason, count in stats['halt_reasons'].items():
            print(f"   - {reason}: {count}æ¬¡")
    
    print("=" * 70 + "\n")


def plot_entropy_analysis(data, sample_ids=None, output_dir='./output', dpi=300, fmt='png', samples_per_figure=5):
    """
    ç»˜åˆ¶ç†µå˜åŒ–ç‡åˆ†æå›¾
    
    å‚æ•°:
        data: å®éªŒæ•°æ®å­—å…¸
        sample_ids: è¦ç»˜åˆ¶çš„æ ·æœ¬IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºç»˜åˆ¶å‰5ä¸ªæ ·æœ¬ï¼Œ"all"è¡¨ç¤ºæ‰€æœ‰æ ·æœ¬
        output_dir: è¾“å‡ºç›®å½•
        dpi: å›¾ç‰‡åˆ†è¾¨ç‡
        fmt: å›¾ç‰‡æ ¼å¼
        samples_per_figure: æ¯å¼ å›¾æ˜¾ç¤ºå¤šå°‘ä¸ªæ ·æœ¬
    """
    results = data['results']
    
    # å¤„ç† sample_ids
    if sample_ids == "all":
        sample_ids = list(range(len(results)))
        print(f"ğŸ“ˆ å°†ç»˜åˆ¶æ‰€æœ‰ {len(results)} ä¸ªæ ·æœ¬çš„ç†µåˆ†æå›¾...")
    elif sample_ids is None:
        sample_ids = list(range(min(5, len(results))))
    
    # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„æ ·æœ¬ID
    sample_ids = [sid for sid in sample_ids if sid < len(results)]
    
    if not sample_ids:
        print("âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æ ·æœ¬IDå¯ä¾›ç»˜åˆ¶")
        return
    
    # åˆ†æ‰¹ç»˜åˆ¶
    total_samples = len(sample_ids)
    num_figures = (total_samples + samples_per_figure - 1) // samples_per_figure
    
    print(f"ğŸ“ˆ æ­£åœ¨ç»˜åˆ¶ {total_samples} ä¸ªæ ·æœ¬ï¼Œåˆ†ä¸º {num_figures} å¼ å›¾...")
    
    setup_chinese_font()
    
    for fig_idx in range(num_figures):
        start_idx = fig_idx * samples_per_figure
        end_idx = min(start_idx + samples_per_figure, total_samples)
        batch_sample_ids = sample_ids[start_idx:end_idx]
        
        print(f"  æ­£åœ¨ç»˜åˆ¶ç¬¬ {fig_idx + 1}/{num_figures} å¼ å›¾ (æ ·æœ¬ {batch_sample_ids})...")
        
        # åˆ›å»ºå­å›¾
        n_samples = len(batch_sample_ids)
        fig, axes = plt.subplots(n_samples, 1, figsize=(16, 5*n_samples))
        
        if n_samples == 1:
            axes = [axes]
        
        for idx, sample_id in enumerate(batch_sample_ids):
            sample = results[sample_id]
            ax = axes[idx]
            
            # æå–æ•°æ®
            probe_history = sample['probe_history']
            
            if not probe_history:
                print(f"    âš ï¸  è­¦å‘Š: æ ·æœ¬ {sample_id} æ²¡æœ‰æ¢é’ˆå†å²æ•°æ®")
                continue
            
            token_positions = [p['token_position'] for p in probe_history]
            entropies = [p['entropy'] for p in probe_history]
            probed_answers = [p['probed_answer'] for p in probe_history]
            stages = [p['stage'] for p in probe_history]
            confidences = [p.get('confidence', 0) for p in probe_history]
            
            ground_truth = str(sample['ground_truth'])
            is_correct = [str(ans) == ground_truth for ans in probed_answers]
            
            # è®¡ç®—ç†µå˜åŒ–ç‡
            entropy_rates = [0]  # ç¬¬ä¸€ä¸ªç‚¹çš„å˜åŒ–ç‡ä¸º0
            for i in range(1, len(entropies)):
                token_diff = token_positions[i] - token_positions[i-1]
                if token_diff > 0:
                    rate = (entropies[i] - entropies[i-1]) / token_diff
                    entropy_rates.append(rate)
                else:
                    entropy_rates.append(0)
            
            # åˆ›å»ºåŒYè½´
            ax2 = ax.twinx()
            
            # ç»˜åˆ¶ç†µå€¼æ›²çº¿
            line1 = ax.plot(token_positions, entropies, 'b-', linewidth=2.5, 
                           label='ç†µå€¼', marker='o', markersize=10, markerfacecolor='lightblue',
                           markeredgecolor='blue', markeredgewidth=2)
            
            # ç»˜åˆ¶ç†µå˜åŒ–ç‡
            if len(token_positions) > 1:
                line2 = ax2.plot(token_positions, entropy_rates, 'g--', linewidth=2, 
                               label='ç†µå˜åŒ–ç‡', marker='s', markersize=7, 
                               markerfacecolor='lightgreen', markeredgecolor='green',
                               markeredgewidth=1.5, alpha=0.8)
            else:
                line2 = []
            
            # æ ‡æ³¨æ¢é’ˆç‚¹
            for i, (pos, ent, ans, correct, stage, conf) in enumerate(
                zip(token_positions, entropies, probed_answers, is_correct, stages, confidences)):
                
                # æ ¹æ®æ­£ç¡®æ€§é€‰æ‹©é¢œè‰²å’Œæ ‡è®°
                color = 'green' if correct else 'red'
                marker = 'o' if correct else 'X'
                
                # ç»˜åˆ¶æ¢é’ˆæ ‡è®°ï¼ˆæ›´å¤§æ›´æ˜æ˜¾ï¼‰
                ax.scatter(pos, ent, s=300, c=color, marker=marker, 
                          edgecolors='black', linewidths=2.5, zorder=5, alpha=0.8)
                
                # æ·»åŠ ç­”æ¡ˆæ ‡æ³¨
                y_offset = 20 if i % 2 == 0 else -35
                annotation_text = f'ç­”æ¡ˆ: {ans}\né˜¶æ®µ: {stage}\nç½®ä¿¡åº¦: {conf:.2f}'
                
                ax.annotate(annotation_text, 
                           xy=(pos, ent), 
                           xytext=(0, y_offset),
                           textcoords='offset points',
                           ha='center',
                           fontsize=9,
                           bbox=dict(boxstyle='round,pad=0.5', 
                                    facecolor=color, 
                                    alpha=0.25,
                                    edgecolor='black',
                                    linewidth=1.5),
                           arrowprops=dict(arrowstyle='->', 
                                         connectionstyle='arc3,rad=0',
                                         color='black',
                                         lw=1.5))
            
            # è®¾ç½®æ ‡é¢˜ - ä½¿ç”¨æ¸…ç†åçš„æ–‡æœ¬
            correct_mark = "âœ“" if sample['correct'] else "âœ—"
            
            # æ¸…ç†æ‰€æœ‰å¯èƒ½åŒ…å«LaTeXçš„æ–‡æœ¬
            question_clean = clean_latex_text(sample["question"][:100])
            gt_clean = clean_latex_text(str(ground_truth))
            pred_clean = clean_latex_text(str(sample["predicted_answer"]))
            
            title_text = (f'æ ·æœ¬ {sample_id} {correct_mark}\n'
                        f'é—®é¢˜: {question_clean}...\n'
                        f'æ­£ç¡®ç­”æ¡ˆ: {gt_clean} | é¢„æµ‹ç­”æ¡ˆ: {pred_clean} | '
                        f'å¹³å‡ç†µ: {sample["avg_entropy"]:.4f} | Tokenæ•°: {sample["tokens_used"]}')
            
            ax.set_title(title_text, fontsize=12, pad=15, fontweight='bold')

            
            # è®¾ç½®æ ‡ç­¾
            ax.set_xlabel('Token ä½ç½®', fontsize=11, fontweight='bold')
            ax.set_ylabel('ç†µå€¼', fontsize=11, color='b', fontweight='bold')
            ax2.set_ylabel('ç†µå˜åŒ–ç‡ (Î”ç†µ/Î”Token)', fontsize=11, color='g', fontweight='bold')
            
            ax.tick_params(axis='y', labelcolor='b', labelsize=10)
            ax2.tick_params(axis='y', labelcolor='g', labelsize=10)
            ax.tick_params(axis='x', labelsize=10)
            
            # æ·»åŠ ç½‘æ ¼
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=1)
            
            # æ·»åŠ æ—©åœæ ‡è®°
            if sample['early_stopped']:
                halt_reason = sample.get('halt_reason', 'unknown')
                ax.axvline(x=token_positions[-1], color='purple', linestyle=':', 
                          linewidth=3, label=f'æ—©åœ: {halt_reason}', alpha=0.7)
            
            # æ·»åŠ é›¶çº¿ï¼ˆç†µå˜åŒ–ç‡ï¼‰
            if len(token_positions) > 1:
                ax2.axhline(y=0, color='gray', linestyle='-', linewidth=1, alpha=0.5)
            
            # åˆå¹¶å›¾ä¾‹
            lines = line1 + line2
            labels = [l.get_label() for l in lines]
            
            # æ·»åŠ è‡ªå®šä¹‰å›¾ä¾‹é¡¹
            correct_patch = mpatches.Patch(color='green', label='âœ“ æ­£ç¡®ç­”æ¡ˆ', alpha=0.7)
            incorrect_patch = mpatches.Patch(color='red', label='âœ— é”™è¯¯ç­”æ¡ˆ', alpha=0.7)
            
            legend_handles = lines + [correct_patch, incorrect_patch]
            
            if sample['early_stopped']:
                early_stop_line = plt.Line2D([0], [0], color='purple', linewidth=3, 
                                            linestyle=':', label=f'æ—©åœ: {halt_reason}')
                legend_handles.append(early_stop_line)
            
            ax.legend(handles=legend_handles, loc='upper left', fontsize=10, 
                     framealpha=0.9, edgecolor='black')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_path = Path(output_dir) / f'entropy_analysis_samples_{start_idx}-{end_idx-1}.{fmt}'
        plt.savefig(output_path, dpi=dpi, bbox_inches='tight', facecolor='white')
        print(f"    âœ“ å·²ä¿å­˜: {output_path}")
        
        plt.close()
    
    print(f"âœ“ æ‰€æœ‰ç†µåˆ†æå›¾å·²ä¿å­˜å®Œæˆ")

def plot_entropy_statistics(data, output_dir='./output', dpi=300, fmt='png'):
    """
    ç»˜åˆ¶æ•´ä½“ç»Ÿè®¡å›¾è¡¨
    """
    print(f"ğŸ“Š æ­£åœ¨ç»˜åˆ¶æ•´ä½“ç»Ÿè®¡å›¾...")
    
    results = data['results']
    setup_chinese_font()
    
    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    # 1. å¹³å‡ç†µåˆ†å¸ƒï¼ˆæ­£ç¡® vs é”™è¯¯ï¼‰
    ax1 = fig.add_subplot(gs[0, 0])
    correct_entropies = [r['avg_entropy'] for r in results if r['correct']]
    incorrect_entropies = [r['avg_entropy'] for r in results if not r['correct']]
    
    ax1.hist([correct_entropies, incorrect_entropies], 
            bins=20, label=['æ­£ç¡®', 'é”™è¯¯'], 
            color=['green', 'red'], alpha=0.6, edgecolor='black')
    ax1.set_xlabel('å¹³å‡ç†µå€¼', fontsize=11, fontweight='bold')
    ax1.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=11, fontweight='bold')
    ax1.set_title('å¹³å‡ç†µåˆ†å¸ƒå¯¹æ¯”', fontsize=12, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    if correct_entropies:
        ax1.axvline(np.mean(correct_entropies), color='green', linestyle='--', 
                   linewidth=2, label=f'æ­£ç¡®å‡å€¼: {np.mean(correct_entropies):.3f}')
    if incorrect_entropies:
        ax1.axvline(np.mean(incorrect_entropies), color='red', linestyle='--', 
                   linewidth=2, label=f'é”™è¯¯å‡å€¼: {np.mean(incorrect_entropies):.3f}')
    ax1.legend(fontsize=9)
    
    # 2. Tokenæ•°é‡ vs å‡†ç¡®æ€§
    ax2 = fig.add_subplot(gs[0, 1])
    token_counts = [r['tokens_used'] for r in results]
    colors = ['green' if r['correct'] else 'red' for r in results]
    
    ax2.scatter(range(len(results)), token_counts, c=colors, alpha=0.6, s=100, edgecolors='black')
    ax2.set_xlabel('æ ·æœ¬ID', fontsize=11, fontweight='bold')
    ax2.set_ylabel('Tokenæ•°é‡', fontsize=11, fontweight='bold')
    ax2.set_title('Tokenä½¿ç”¨é‡åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    correct_patch = mpatches.Patch(color='green', label='æ­£ç¡®', alpha=0.6)
    incorrect_patch = mpatches.Patch(color='red', label='é”™è¯¯', alpha=0.6)
    ax2.legend(handles=[correct_patch, incorrect_patch], fontsize=10)
    
    # 3. å‡†ç¡®ç‡é¥¼å›¾
    ax3 = fig.add_subplot(gs[0, 2])
    correct_count = sum(1 for r in results if r['correct'])
    incorrect_count = len(results) - correct_count
    
    colors_pie = ['green', 'red']
    explode = (0.05, 0.05)
    ax3.pie([correct_count, incorrect_count], labels=['æ­£ç¡®', 'é”™è¯¯'], 
           autopct='%1.1f%%', startangle=90, colors=colors_pie, explode=explode,
           textprops={'fontsize': 11, 'fontweight': 'bold'})
    ax3.set_title(f'å‡†ç¡®ç‡: {correct_count}/{len(results)}', fontsize=12, fontweight='bold')
    
    # 4. ç†µå˜åŒ–è¶‹åŠ¿ï¼ˆæ‰€æœ‰æ ·æœ¬çš„å¹³å‡ï¼‰
    ax4 = fig.add_subplot(gs[1, :2])
    max_probes = max(len(r['entropy_history']) for r in results if r['entropy_history'])
    entropy_by_position = [[] for _ in range(max_probes)]
    
    for r in results:
        for i, ent in enumerate(r['entropy_history']):
            if i < max_probes:
                entropy_by_position[i].append(ent)
    
    avg_entropies = [np.mean(e) if e else 0 for e in entropy_by_position]
    std_entropies = [np.std(e) if e else 0 for e in entropy_by_position]
    
    x = range(len(avg_entropies))
    ax4.plot(x, avg_entropies, 'b-', linewidth=3, marker='o', markersize=8, label='å¹³å‡ç†µ')
    ax4.fill_between(x, 
                    np.array(avg_entropies) - np.array(std_entropies),
                    np.array(avg_entropies) + np.array(std_entropies),
                    alpha=0.3, label='æ ‡å‡†å·®èŒƒå›´')
    ax4.set_xlabel('æ¢é’ˆä½ç½®', fontsize=11, fontweight='bold')
    ax4.set_ylabel('å¹³å‡ç†µå€¼', fontsize=11, fontweight='bold')
    ax4.set_title('ç†µå€¼éšæ¢é’ˆä½ç½®çš„å˜åŒ–è¶‹åŠ¿ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰', fontsize=12, fontweight='bold')
    ax4.legend(fontsize=10)
    ax4.grid(True, alpha=0.3)
    
    # 5. é˜¶æ®µåˆ†å¸ƒç»Ÿè®¡
    ax5 = fig.add_subplot(gs[1, 2])
    stage_counts = {}
    for r in results:
        for stage, count in r['stage_distribution'].items():
            stage_counts[stage] = stage_counts.get(stage, 0) + count
    
    stages = list(stage_counts.keys())
    counts = list(stage_counts.values())
    colors_stage = plt.cm.Set3(range(len(stages)))
    
    bars = ax5.bar(stages, counts, color=colors_stage, alpha=0.7, edgecolor='black', linewidth=1.5)
    ax5.set_xlabel('æ¨ç†é˜¶æ®µ', fontsize=11, fontweight='bold')
    ax5.set_ylabel('æ¢é’ˆæ¬¡æ•°', fontsize=11, fontweight='bold')
    ax5.set_title('æ¨ç†é˜¶æ®µåˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(count)}',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # 6. æ—©åœç»Ÿè®¡
    ax6 = fig.add_subplot(gs[2, 0])
    early_stopped = sum(1 for r in results if r['early_stopped'])
    not_stopped = len(results) - early_stopped
    
    ax6.bar(['æ—©åœ', 'æœªæ—©åœ'], [early_stopped, not_stopped], 
           color=['purple', 'gray'], alpha=0.7, edgecolor='black', linewidth=1.5)
    ax6.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=11, fontweight='bold')
    ax6.set_title(f'æ—©åœç»Ÿè®¡ (æ—©åœç‡: {early_stopped/len(results)*100:.1f}%)', 
                 fontsize=12, fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='y')
    
    for i, v in enumerate([early_stopped, not_stopped]):
        ax6.text(i, v, str(v), ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # 7. ç”Ÿæˆæ—¶é—´åˆ†å¸ƒ
    ax7 = fig.add_subplot(gs[2, 1])
    gen_times = [r['generation_time'] for r in results]
    ax7.hist(gen_times, bins=20, color='orange', alpha=0.7, edgecolor='black')
    ax7.axvline(np.mean(gen_times), color='red', linestyle='--', linewidth=2,
               label=f'å¹³å‡: {np.mean(gen_times):.2f}s')
    ax7.set_xlabel('ç”Ÿæˆæ—¶é—´ (ç§’)', fontsize=11, fontweight='bold')
    ax7.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=11, fontweight='bold')
    ax7.set_title('ç”Ÿæˆæ—¶é—´åˆ†å¸ƒ', fontsize=12, fontweight='bold')
    ax7.legend(fontsize=10)
    ax7.grid(True, alpha=0.3)
    
    # 8. Tokenæ•°é‡ vs ç†µå€¼æ•£ç‚¹å›¾
    ax8 = fig.add_subplot(gs[2, 2])
    tokens = [r['tokens_used'] for r in results]
    entropies = [r['avg_entropy'] for r in results]
    colors_scatter = ['green' if r['correct'] else 'red' for r in results]
    
    ax8.scatter(tokens, entropies, c=colors_scatter, alpha=0.6, s=100, edgecolors='black')
    ax8.set_xlabel('Tokenæ•°é‡', fontsize=11, fontweight='bold')
    ax8.set_ylabel('å¹³å‡ç†µå€¼', fontsize=11, fontweight='bold')
    ax8.set_title('Tokenæ•°é‡ vs å¹³å‡ç†µå€¼', fontsize=12, fontweight='bold')
    ax8.grid(True, alpha=0.3)
    ax8.legend(handles=[correct_patch, incorrect_patch], fontsize=10)
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle('HALT-CoT å®éªŒæ•´ä½“ç»Ÿè®¡åˆ†æ', fontsize=16, fontweight='bold', y=0.995)
    
    # ä¿å­˜å›¾ç‰‡
    output_path = Path(output_dir) / f'entropy_statistics.{fmt}'
    plt.savefig(output_path, dpi=dpi, bbox_inches='tight', facecolor='white')
    print(f"âœ“ ç»Ÿè®¡å›¾å·²ä¿å­˜: {output_path}")
    
    plt.close()


def generate_summary_report(data, output_dir='./output'):
    """ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æŠ¥å‘Š"""
    print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š...")
    
    results = data['results']
    stats = data['statistics']
    
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("HALT-CoT å®éªŒè¯¦ç»†æŠ¥å‘Š")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # åŸºæœ¬ä¿¡æ¯
    report_lines.append("ã€å®éªŒé…ç½®ã€‘")
    report_lines.append(f"æ¨¡å‹: {data['experiment_info']['model']}")
    report_lines.append(f"æ—¶é—´æˆ³: {data['experiment_info']['timestamp']}")
    report_lines.append(f"æ ·æœ¬å¤§å°: {data['experiment_info']['sample_size']}")
    report_lines.append(f"æ£€æµ‹æ¨¡å¼: {data['experiment_info']['stage_detection_mode']}")
    report_lines.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    report_lines.append("ã€æ•´ä½“ç»Ÿè®¡ã€‘")
    report_lines.append(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    report_lines.append(f"æ­£ç¡®æ ·æœ¬: {stats['correct_samples']}")
    report_lines.append(f"å‡†ç¡®ç‡: {stats['accuracy']*100:.2f}%")
    report_lines.append(f"æ—©åœç‡: {stats['early_stop_rate']*100:.2f}%")
    report_lines.append(f"å¹³å‡ç”¨æ—¶: {stats['avg_time_per_sample']:.2f}ç§’/æ ·æœ¬")
    report_lines.append(f"æ€»ç”¨æ—¶: {stats['total_time']:.2f}ç§’")
    report_lines.append(f"å¹³å‡Token: {stats['avg_tokens_per_sample']:.2f}ä¸ª/æ ·æœ¬")
    report_lines.append(f"TokenèŒƒå›´: {stats['min_tokens']} - {stats['max_tokens']}")
    report_lines.append(f"å¹³å‡ç†µ: {stats['avg_entropy']:.4f}")
    report_lines.append("")
    
    # æ—©åœåŸå› 
    if stats.get('halt_reasons'):
        report_lines.append("ã€æ—©åœåŸå› åˆ†å¸ƒã€‘")
        for reason, count in stats['halt_reasons'].items():
            report_lines.append(f"  - {reason}: {count}æ¬¡")
        report_lines.append("")
    
    # æ ·æœ¬è¯¦æƒ…
    report_lines.append("ã€æ ·æœ¬è¯¦æƒ…ã€‘")
    for i, r in enumerate(results[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªæ ·æœ¬
        status = "âœ“" if r['correct'] else "âœ—"
        report_lines.append(f"\næ ·æœ¬ {i} {status}")
        report_lines.append(f"  é—®é¢˜: {r['question'][:80]}...")
        report_lines.append(f"  æ­£ç¡®ç­”æ¡ˆ: {r['ground_truth']}")
        report_lines.append(f"  é¢„æµ‹ç­”æ¡ˆ: {r['predicted_answer']}")
        report_lines.append(f"  Tokenæ•°: {r['tokens_used']}")
        report_lines.append(f"  å¹³å‡ç†µ: {r['avg_entropy']:.4f}")
        report_lines.append(f"  æ—©åœ: {'æ˜¯' if r['early_stopped'] else 'å¦'}")
        if r['early_stopped']:
            report_lines.append(f"  æ—©åœåŸå› : {r['halt_reason']}")
    
    if len(results) > 10:
        report_lines.append(f"\n... è¿˜æœ‰ {len(results) - 10} ä¸ªæ ·æœ¬æœªæ˜¾ç¤º")
    
    report_lines.append("")
    report_lines.append("=" * 80)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path(output_dir) / 'summary_report.txt'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"âœ“ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸš€ HALT-CoT ç†µå˜åŒ–ç‡åˆ†æè„šæœ¬")
    print("=" * 70 + "\n")
    
    # 1. åŠ è½½æ•°æ®
    data = load_data(JSON_FILE_PATH)
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    create_output_dir(OUTPUT_DIR)
    
    # 3. æ‰“å°å®éªŒä¿¡æ¯
    print_experiment_info(data)
    
    # 4. ç»˜åˆ¶è¯¦ç»†çš„ç†µåˆ†æå›¾
    if SAMPLES_TO_PLOT:
        plot_entropy_analysis(data, sample_ids=SAMPLES_TO_PLOT, 
                            output_dir=OUTPUT_DIR, dpi=DPI, fmt=FIGURE_FORMAT,
                            samples_per_figure=SAMPLES_PER_FIGURE)
    
    # 5. ç»˜åˆ¶æ•´ä½“ç»Ÿè®¡å›¾
    if PLOT_ALL_STATISTICS:
        plot_entropy_statistics(data, output_dir=OUTPUT_DIR, dpi=DPI, fmt=FIGURE_FORMAT)
    
    # 6. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    generate_summary_report(data, output_dir=OUTPUT_DIR)
    
    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("=" * 70 + "\n")



if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
